{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Mar 29 2022 22:56:14\n"
     ]
    }
   ],
   "source": [
    "from safe_control_gym.envs.manipulators.manipulator import BaseManipulator\n",
    "import numpy as np \n",
    "import random \n",
    "from time import sleep\n",
    "from safe_control_gym.envs.benchmark_env import Cost, Task\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "Version = 4.1 Metal - 76.3\n",
      "Vendor = Apple\n",
      "Renderer = Apple M1 Pro\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    }
   ],
   "source": [
    "urdf_path = \"../safe_control_gym/envs/manipulators/assets/franka_panda/panda.urdf\"\n",
    "control_mode = \"torque\"\n",
    "target_space = \"joint\"\n",
    "\n",
    "controlled_joint_indices = [6]\n",
    "observed_link_indices = [7]\n",
    "observed_link_state_keys = [\"position\"]\n",
    "goal = [{\n",
    "    \"position\": [None for i in range(13)]\n",
    "}]\n",
    "\n",
    "# orientation_goal = [0.603, 0.3687, -0.3697, 0.6026]\n",
    "position_goal = [0.03692, 0, 0.973]\n",
    "# position_goal = [0.0, 0, 1]\n",
    "goal[0][\"position\"][observed_link_indices[0]] = position_goal\n",
    "goal_type = \"point\"\n",
    "\n",
    "env = BaseManipulator(\n",
    "    urdf_path,\n",
    "    control_mode,\n",
    "    target_space,\n",
    "    controlled_joint_indices = controlled_joint_indices,\n",
    "    observed_link_indices = observed_link_indices, \n",
    "    observed_link_state_keys = observed_link_state_keys,\n",
    "    goal = goal,\n",
    "    goal_type = goal_type,\n",
    "    cost = Cost.RL_REWARD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    env._joint_apply_action(6, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4]\n",
      "obs: (0.026117555797100067, -0.0003685210831463337, 1.1163884401321411)\n",
      "reward: -1.545594054181874\n",
      "[-3]\n",
      "obs: (0.03053838014602661, -0.00043593964073807, 1.1144816875457764)\n",
      "reward: -1.4829924704048785\n",
      "[-3]\n",
      "obs: (0.03717472404241562, -0.0005226460634730756, 1.1114221811294556)\n",
      "reward: -1.3919955123534429\n",
      "[-2]\n",
      "obs: (0.04525262862443924, -0.0006275356863625348, 1.1067513227462769)\n",
      "reward: -1.4271148705707866\n",
      "[-2]\n",
      "obs: (0.05174412578344345, -0.0007121260860003531, 1.1020947694778442)\n",
      "reward: -1.4463102134728807\n",
      "[-3]\n",
      "obs: (0.05680356174707413, -0.0007780581945553422, 1.0977861881256104)\n",
      "reward: -1.4544780806723985\n",
      "[-3]\n",
      "obs: (0.06286358833312988, -0.0008568663033656776, 1.0916376113891602)\n",
      "reward: -1.4543806602565574\n",
      "[-4]\n",
      "obs: (0.06944775581359863, -0.0009424532181583345, 1.083307147026062)\n",
      "reward: -1.43777356057819\n",
      "[-3]\n",
      "obs: (0.07732827961444855, -0.0010449215769767761, 1.0694775581359863)\n",
      "reward: -1.3793075932741168\n",
      "[-2]\n",
      "obs: (0.08337040990591049, -0.0011187001364305615, 1.0524965524673462)\n",
      "reward: -1.2706566250968727\n",
      "[-4]\n",
      "obs: (0.08600303530693054, -0.001145864836871624, 1.0360435247421265)\n",
      "reward: -1.1327242488592866\n",
      "[-3]\n",
      "obs: (0.08566893637180328, -0.001119718886911869, 1.01390540599823)\n",
      "reward: -0.9077406125694515\n",
      "[-4]\n",
      "obs: (0.08332053571939468, -0.001118671614676714, 1.014196753501892)\n",
      "reward: -0.887159608359635\n",
      "[-2]\n",
      "obs: (0.08211706578731537, -0.0011179225984960794, 1.0143429040908813)\n",
      "reward: -0.8765789247669281\n",
      "[-2]\n",
      "obs: (0.08205888420343399, -0.0011177120031788945, 1.0143498182296753)\n",
      "reward: -0.8760641443628819\n",
      "[-4]\n",
      "obs: (0.08205857872962952, -0.001117601408623159, 1.0143498182296753)\n",
      "reward: -0.8760599836792798\n",
      "[-4]\n",
      "obs: (0.08205834031105042, -0.0011175350518897176, 1.0143498182296753)\n",
      "reward: -0.8760569359261544\n",
      "[-4]\n",
      "obs: (0.0820581391453743, -0.0011174838291481137, 1.0143499374389648)\n",
      "reward: -0.8760556041348727\n",
      "[-2]\n",
      "obs: (0.08205799013376236, -0.0011174432002007961, 1.0143499374389648)\n",
      "reward: -0.8760537077292802\n",
      "[-2]\n",
      "obs: (0.08205786347389221, -0.0011174111859872937, 1.0143499374389648)\n",
      "reward: -0.8760521209884436\n",
      "[-3]\n",
      "obs: (0.08205777406692505, -0.0011173856910318136, 1.0143499374389648)\n",
      "reward: -0.8760509719692172\n",
      "[-3]\n",
      "obs: (0.08205770701169968, -0.0011173656675964594, 1.0143499374389648)\n",
      "reward: -0.87605010118261\n",
      "[-4]\n",
      "obs: (0.0820576399564743, -0.001117349835112691, 1.0143499374389648)\n",
      "reward: -0.8760492723055185\n",
      "[-3]\n",
      "obs: (0.08205760270357132, -0.0011173373786732554, 1.0143499374389648)\n",
      "reward: -0.8760487752120943\n",
      "[-2]\n",
      "obs: (0.08205756545066833, -0.001117327599786222, 1.0143499374389648)\n",
      "reward: -0.8760483048941942\n",
      "[-2]\n",
      "obs: (0.08205753564834595, -0.0011173197999596596, 1.0143499374389648)\n",
      "reward: -0.8760479288727047\n",
      "[-4]\n",
      "obs: (0.08205751329660416, -0.0011173136299476027, 1.0143499374389648)\n",
      "reward: -0.8760476436551662\n",
      "[-3]\n",
      "obs: (0.08205749839544296, -0.0011173088569194078, 1.0143499374389648)\n",
      "reward: -0.8760474469132723\n",
      "[-3]\n",
      "obs: (0.08205747604370117, -0.0011173050152137876, 1.0143499374389648)\n",
      "reward: -0.8760471849787982\n",
      "[-2]\n",
      "obs: (0.08205746859312057, -0.0011173021048307419, 1.0143499374389648)\n",
      "reward: -0.8760470813691618\n",
      "[-3]\n",
      "obs: (0.08205746114253998, -0.0011172996601089835, 1.0143499374389648)\n",
      "reward: -0.8760469824161382\n",
      "[-3]\n",
      "obs: (0.08205746114253998, -0.0011172979138791561, 1.0143499374389648)\n",
      "reward: -0.8760469649538399\n",
      "[-2]\n",
      "obs: (0.08205745369195938, -0.0011172964004799724, 1.0143499374389648)\n",
      "reward: -0.8760468753140421\n",
      "[-3]\n",
      "obs: (0.08205744624137878, -0.001117295236326754, 1.0143499374389648)\n",
      "reward: -0.876046789166704\n",
      "[-4]\n",
      "obs: (0.08205744624137878, -0.0011172944214195013, 1.0143499374389648)\n",
      "reward: -0.8760467810176314\n",
      "[-4]\n",
      "obs: (0.08205744624137878, -0.0011172936065122485, 1.0143499374389648)\n",
      "reward: -0.8760467728685589\n",
      "[-3]\n",
      "obs: (0.08205743879079819, -0.0011172931408509612, 1.0143499374389648)\n",
      "reward: -0.8760466937061401\n",
      "[-3]\n",
      "obs: (0.08205743879079819, -0.001117292675189674, 1.0143499374389648)\n",
      "reward: -0.8760466890495272\n",
      "[-3]\n",
      "obs: (0.08205743879079819, -0.0011172923259437084, 1.0143499374389648)\n",
      "reward: -0.8760466855570676\n",
      "[-4]\n",
      "obs: (0.08205743134021759, -0.0011172920931130648, 1.0143499374389648)\n",
      "reward: -0.8760466087229551\n",
      "[-3]\n",
      "obs: (0.08205743134021759, -0.0011172918602824211, 1.0143499374389648)\n",
      "reward: -0.8760466063946487\n",
      "[-2]\n",
      "obs: (0.08205743134021759, -0.0011172916274517775, 1.0143499374389648)\n",
      "reward: -0.8760466040663423\n",
      "[-2]\n",
      "obs: (0.08205743134021759, -0.0011172915110364556, 1.0143499374389648)\n",
      "reward: -0.876046602902189\n",
      "[-3]\n",
      "obs: (0.08205743134021759, -0.0011172913946211338, 1.0143499374389648)\n",
      "reward: -0.8760466017380358\n",
      "[-3]\n",
      "obs: (0.08205743134021759, -0.0011172913946211338, 1.0143499374389648)\n",
      "reward: -0.8760466017380358\n",
      "[-4]\n",
      "obs: (0.08205743134021759, -0.001117291278205812, 1.0143499374389648)\n",
      "reward: -0.8760466005738826\n",
      "[-3]\n",
      "obs: (0.08205743134021759, -0.0011172911617904902, 1.0143499374389648)\n",
      "reward: -0.8760465994097294\n",
      "[-3]\n",
      "obs: (0.08205743134021759, -0.0011172911617904902, 1.0143499374389648)\n",
      "reward: -0.8760465994097294\n",
      "[-3]\n",
      "obs: (0.08205743134021759, -0.0011172911617904902, 1.0143499374389648)\n",
      "reward: -0.8760465994097294\n",
      "[-3]\n",
      "obs: (0.08205743134021759, -0.0011172911617904902, 1.0143499374389648)\n",
      "reward: -0.8760465994097294\n"
     ]
    }
   ],
   "source": [
    "action_space = [-4, -2, -3, 3, 2, 4]\n",
    "# action_space = [-400]\n",
    "\n",
    "for i in range(50):\n",
    "#     action_index = env.n_joints-2\n",
    "    action_index = 6\n",
    "    # action_list = np.zeros(env.n_joints)\n",
    "    action_list = [random.choice(action_space)]    \n",
    "    \n",
    "    # action_list = [200]\n",
    "    obs,reward, done,info = env.step(action_list)\n",
    "    # if i%5== 0 :\n",
    "    print(action_list)\n",
    "    print(f\"obs: {obs}\") # link position\n",
    "    print(f\"reward: {reward}\")\n",
    "  #   print(f\"angular velocity: {obs}\") # velocity\n",
    "                \n",
    "    sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.n_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_states =  env._pb_client.getLinkState(env.robot,\n",
    "                                            linkIndex=7,\n",
    "                                            computeLinkVelocity=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.088, -4.504557837847756e-13, 0.953),\n",
       " (1.0, 0.0, 0.0, 4.896583138958022e-12),\n",
       " (0.0, 0.0, 0.08),\n",
       " (0.0, 0.0, 0.0, 1.0),\n",
       " (0.08799999952316284, 3.329974913434536e-13, 1.0329999923706055),\n",
       " (1.0, 0.0, 0.0, 4.896583138958022e-12),\n",
       " (0.0, -0.0, 0.0),\n",
       " (0.0, -0.0, 0.0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only extract world positions\n",
    "link_states_dict = {\n",
    "    \"position\": link_states[4],\n",
    "    \"orientation\": link_states[5],\n",
    "    \"linear_vel\": link_states[6],\n",
    "    \"angular_vel\": link_states[7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'position': (0.0010245442390441895,\n",
       "  -1.3434328138828278e-06,\n",
       "  1.0330008268356323),\n",
       " 'orientation': (0.6968757510185242,\n",
       "  -0.11944480240345001,\n",
       "  0.025003790855407715,\n",
       "  0.7067332863807678),\n",
       " 'linear_vel': (0.16140478353054072,\n",
       "  -0.021992152304630256,\n",
       "  -0.7820516002624263),\n",
       " 'angular_vel': (2.675834999501902, 19.791254360890452, -0.004319621550829485)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_states_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.link_states[\"position\"][1] = [4,3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point \n",
    "# 1.0330\n",
    "\n",
    "# orientation:\n",
    "# (0.6034725904464722, 0.3687118589878082, -0.3697621822357178, 0.6026179194450378)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces \n",
    "# env.observation_space = spaces.Box(-np.inf, np.inf, (14,), dtype=np.float32)\n",
    "\n",
    "# orientation use quaternion so 4 dim \n",
    "\n",
    "# action_space = [spaces.Discrete(1) for i in range(env.n_joints-1)]\n",
    "# action_space.append(spaces.Box(-400, 400, shape=(1,), dtype=np.float32))\n",
    "\n",
    "# env.action_space = spaces.Tuple(\n",
    "#     action_space\n",
    "# )\n",
    "\n",
    "# dim dict\n",
    "dim_dict = {\n",
    "    \"position\":3,\n",
    "    \"orientation\" : 4 \n",
    "}\n",
    "env.action_space = spaces.Box(-4, 4, (len(env.controlled_joint_indices),), dtype=np.float32)\n",
    "\n",
    "# TODO expand to multiple state keys \n",
    "env.observation_space = spaces.Box(-np.inf, np.inf, (dim_dict[observed_link_state_keys[0]], ), dtype=np.float32)\n",
    "\n",
    "# env.observation_space = spaces.Dict({\n",
    "#       'position': spaces.Box(-np.inf, np.inf, (env.n_joints, 3), dtype=np.float32),\n",
    "#       'orientation': spaces.Box(-np.inf, np.inf, (env.n_joints, 4), dtype=np.float32),\n",
    "#       'linear_vel': spaces.Box(-np.inf, np.inf, (env.n_joints, 3), dtype=np.float32),\n",
    "#       'angular_vel': spaces.Box(-np.inf, np.inf, (env.n_joints, 3), dtype=np.float32),\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DDPG \n",
    "import gym \n",
    "\n",
    "ddpg_model = DDPG('MlpPolicy', env, verbose=1)\n",
    "ddpg_model.learn(total_timesteps=10000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 161  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.186335e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 4.17e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.76e+06     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 1.96e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.56e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 143           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 42            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016106339 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 3.97e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.95e+06      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000201     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 3.94e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 147           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 55            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00075042015 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 2.66e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.42e+06      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000873     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.92e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 149           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 68            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1289976e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -1.19e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.77e+06      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -5.07e-05     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 5.59e+06      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fd7facdfb50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ppo_model = PPO('MlpPolicy', env, verbose=1)\n",
    "ppo_model.learn(total_timesteps=10000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000027?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate_policy\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000027?line=1'>2</a>\u001b[0m mean_reward, std_reward \u001b[39m=\u001b[39m evaluate_policy(model, model\u001b[39m.\u001b[39;49mget_env(), n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:87\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py?line=84'>85</a>\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py?line=85'>86</a>\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(observations, state\u001b[39m=\u001b[39mstates, episode_start\u001b[39m=\u001b[39mepisode_starts, deterministic\u001b[39m=\u001b[39mdeterministic)\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py?line=86'>87</a>\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py?line=87'>88</a>\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py?line=88'>89</a>\u001b[0m     current_lengths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=154'>155</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=155'>156</a>\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=156'>157</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=157'>158</a>\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=158'>159</a>\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=159'>160</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=160'>161</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=161'>162</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=42'>43</a>\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=43'>44</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=44'>45</a>\u001b[0m         )\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=45'>46</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=46'>47</a>\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=47'>48</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=88'>89</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=89'>90</a>\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=90'>91</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py:187\u001b[0m, in \u001b[0;36mBaseManipulator.step\u001b[0;34m(self, action_list)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=183'>184</a>\u001b[0m             applied_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_mapping_torque(action) \n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=184'>185</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_joint_apply_action(joint_index, applied_action)\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=186'>187</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pb_client\u001b[39m.\u001b[39;49mstepSimulation()\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=188'>189</a>\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_observation()\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=189'>190</a>\u001b[0m \u001b[39m# reward = random.choice(self.reward_space)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Iteration:0\n",
      "action: [1.1000601]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.79957540325822\n",
      "\n",
      "None\n",
      "Iteration:1\n",
      "action: [-0.32491696]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.7991680369433\n",
      "\n",
      "None\n",
      "Iteration:2\n",
      "action: [-0.8704787]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.79977895544843\n",
      "\n",
      "None\n",
      "Iteration:3\n",
      "action: [1.1605697]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80048507258297\n",
      "\n",
      "None\n",
      "Iteration:4\n",
      "action: [-0.37423402]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80006193199196\n",
      "\n",
      "None\n",
      "Iteration:5\n",
      "action: [-0.480774]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8007791376859\n",
      "\n",
      "None\n",
      "Iteration:6\n",
      "action: [0.24636999]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80136630746537\n",
      "\n",
      "None\n",
      "Iteration:7\n",
      "action: [1.3087251]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80126138815658\n",
      "\n",
      "None\n",
      "Iteration:8\n",
      "action: [-0.14067285]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8008849010058\n",
      "\n",
      "None\n",
      "Iteration:9\n",
      "action: [-0.31368503]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.801361738164\n",
      "\n",
      "None\n",
      "Iteration:10\n",
      "action: [0.24051316]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80219451516868\n",
      "\n",
      "None\n",
      "Iteration:11\n",
      "action: [1.200896]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80164477291518\n",
      "\n",
      "None\n",
      "Iteration:12\n",
      "action: [-0.57774085]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80135030035862\n",
      "\n",
      "None\n",
      "Iteration:13\n",
      "action: [0.5247166]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80183473361657\n",
      "\n",
      "None\n",
      "Iteration:14\n",
      "action: [1.4443746]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80153187915684\n",
      "\n",
      "None\n",
      "Iteration:15\n",
      "action: [-0.01787182]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8009842032753\n",
      "\n",
      "None\n",
      "Iteration:16\n",
      "action: [-0.88154846]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80171447658913\n",
      "\n",
      "None\n",
      "Iteration:17\n",
      "action: [-0.7551471]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80230895143004\n",
      "\n",
      "None\n",
      "Iteration:18\n",
      "action: [0.37241617]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8031405642815\n",
      "\n",
      "None\n",
      "Iteration:19\n",
      "action: [0.11598232]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8025906183012\n",
      "\n",
      "None\n",
      "Iteration:20\n",
      "action: [-0.63362485]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80231104690583\n",
      "\n",
      "None\n",
      "Iteration:21\n",
      "action: [-0.48766276]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80278802958318\n",
      "\n",
      "None\n",
      "Iteration:22\n",
      "action: [0.3912101]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80362086479553\n",
      "\n",
      "None\n",
      "Iteration:23\n",
      "action: [0.60071695]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80351588727908\n",
      "\n",
      "None\n",
      "Iteration:24\n",
      "action: [-0.15961751]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80288610949182\n",
      "\n",
      "None\n",
      "Iteration:25\n",
      "action: [0.13778666]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8036162372865\n",
      "\n",
      "None\n",
      "Iteration:26\n",
      "action: [-0.6299992]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80317173448393\n",
      "\n",
      "None\n",
      "Iteration:27\n",
      "action: [0.38605353]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8037848066725\n",
      "\n",
      "None\n",
      "Iteration:28\n",
      "action: [-0.69444656]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80334050759674\n",
      "\n",
      "None\n",
      "Iteration:29\n",
      "action: [-0.771023]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.803826949019\n",
      "\n",
      "None\n",
      "Iteration:30\n",
      "action: [-0.77630806]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80465262468906\n",
      "\n",
      "None\n",
      "Iteration:31\n",
      "action: [0.5780364]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8052608947456\n",
      "\n",
      "None\n",
      "Iteration:32\n",
      "action: [-1.1517241]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80494933824056\n",
      "\n",
      "None\n",
      "Iteration:33\n",
      "action: [1.5017896]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8055621775985\n",
      "\n",
      "None\n",
      "Iteration:34\n",
      "action: [1.5720952]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8051178494189\n",
      "\n",
      "None\n",
      "Iteration:35\n",
      "action: [0.34198266]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.804592554383\n",
      "\n",
      "None\n",
      "Iteration:36\n",
      "action: [-0.18720342]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8043043682538\n",
      "\n",
      "None\n",
      "Iteration:37\n",
      "action: [0.67488146]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80478123451584\n",
      "\n",
      "None\n",
      "Iteration:38\n",
      "action: [1.0274813]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8044708130602\n",
      "\n",
      "None\n",
      "Iteration:39\n",
      "action: [-0.5754711]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80406469821\n",
      "\n",
      "None\n",
      "Iteration:40\n",
      "action: [0.74701554]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80466839896516\n",
      "\n",
      "None\n",
      "Iteration:41\n",
      "action: [0.46543]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80411216655747\n",
      "\n",
      "None\n",
      "Iteration:42\n",
      "action: [0.616027]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80382526099683\n",
      "\n",
      "None\n",
      "Iteration:43\n",
      "action: [-1.1061693]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80341035678984\n",
      "\n",
      "None\n",
      "Iteration:44\n",
      "action: [-0.8836494]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80402136260645\n",
      "\n",
      "None\n",
      "Iteration:45\n",
      "action: [-1.3515222]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80460084897467\n",
      "\n",
      "None\n",
      "Iteration:46\n",
      "action: [1.145408]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80543254913763\n",
      "\n",
      "None\n",
      "Iteration:47\n",
      "action: [-0.87584776]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80534232726322\n",
      "\n",
      "None\n",
      "Iteration:48\n",
      "action: [-0.9041307]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8054739929922\n",
      "\n",
      "None\n",
      "Iteration:49\n",
      "action: [-0.73883766]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80559692757205\n",
      "\n",
      "None\n",
      "Iteration:50\n",
      "action: [-0.16532956]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80627411549912\n",
      "\n",
      "None\n",
      "Iteration:51\n",
      "action: [-0.45035917]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80719333088027\n",
      "\n",
      "None\n",
      "Iteration:52\n",
      "action: [0.07374232]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8080554736499\n",
      "\n",
      "None\n",
      "Iteration:53\n",
      "action: [-0.9178448]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80750544035809\n",
      "\n",
      "None\n",
      "Iteration:54\n",
      "action: [-0.09433405]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8082225296367\n",
      "\n",
      "None\n",
      "Iteration:55\n",
      "action: [0.91872025]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80881720820443\n",
      "\n",
      "None\n",
      "Iteration:56\n",
      "action: [-0.5368103]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8085133060068\n",
      "\n",
      "None\n",
      "Iteration:57\n",
      "action: [-0.6854749]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80899954370224\n",
      "\n",
      "None\n",
      "Iteration:58\n",
      "action: [-1.2082434]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80982524847613\n",
      "\n",
      "None\n",
      "Iteration:59\n",
      "action: [0.27286842]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81041861737148\n",
      "\n",
      "None\n",
      "Iteration:60\n",
      "action: [-2.0287123]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81011442413555\n",
      "\n",
      "None\n",
      "Iteration:61\n",
      "action: [0.8526439]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81073672223837\n",
      "\n",
      "None\n",
      "Iteration:62\n",
      "action: [-0.11978073]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81029993195088\n",
      "\n",
      "None\n",
      "Iteration:63\n",
      "action: [-1.2774647]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81089816118592\n",
      "\n",
      "None\n",
      "Iteration:64\n",
      "action: [0.39134318]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8114928979613\n",
      "\n",
      "None\n",
      "Iteration:65\n",
      "action: [0.5730296]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81150715883822\n",
      "\n",
      "None\n",
      "Iteration:66\n",
      "action: [0.19699106]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81101149150172\n",
      "\n",
      "None\n",
      "Iteration:67\n",
      "action: [-0.35385385]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8105965581909\n",
      "\n",
      "None\n",
      "Iteration:68\n",
      "action: [0.20600933]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8111926628463\n",
      "\n",
      "None\n",
      "Iteration:69\n",
      "action: [-0.4728766]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8107630321011\n",
      "\n",
      "None\n",
      "Iteration:70\n",
      "action: [-0.15405919]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81136123223231\n",
      "\n",
      "None\n",
      "Iteration:71\n",
      "action: [0.54345125]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81195593990387\n",
      "\n",
      "None\n",
      "Iteration:72\n",
      "action: [0.684648]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81164458712564\n",
      "\n",
      "None\n",
      "Iteration:73\n",
      "action: [1.3995166]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81123084707187\n",
      "\n",
      "None\n",
      "Iteration:74\n",
      "action: [0.9116049]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.810696791783\n",
      "\n",
      "None\n",
      "Iteration:75\n",
      "action: [0.04740268]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81041591071525\n",
      "\n",
      "None\n",
      "Iteration:76\n",
      "action: [0.8307592]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80988173901105\n",
      "\n",
      "None\n",
      "Iteration:77\n",
      "action: [-2.1865785]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80960079973565\n",
      "\n",
      "None\n",
      "Iteration:78\n",
      "action: [1.7931459]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8101349132322\n",
      "\n",
      "None\n",
      "Iteration:79\n",
      "action: [-0.43068886]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.80982524847613\n",
      "\n",
      "None\n",
      "Iteration:80\n",
      "action: [1.1321412]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81030426842162\n",
      "\n",
      "None\n",
      "Iteration:81\n",
      "action: [-1.4172871]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.8099940797966\n",
      "\n",
      "None\n",
      "Iteration:82\n",
      "action: [-0.10108504]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81048805911095\n",
      "\n",
      "None\n",
      "Iteration:83\n",
      "action: [-2.7923028]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.072, 0.0, 1.084]\n",
      "reward: -146.81130625509658\n",
      "\n",
      "None\n",
      "Iteration:84\n",
      "action: [-0.02272157]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.4937351815589\n",
      "\n",
      "None\n",
      "Iteration:85\n",
      "action: [0.16901925]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49492529539393\n",
      "\n",
      "None\n",
      "Iteration:86\n",
      "action: [-0.32071057]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49474159201608\n",
      "\n",
      "None\n",
      "Iteration:87\n",
      "action: [-0.77795523]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49488941037097\n",
      "\n",
      "None\n",
      "Iteration:88\n",
      "action: [0.80445224]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49501158825123\n",
      "\n",
      "None\n",
      "Iteration:89\n",
      "action: [-0.9145013]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49486388631166\n",
      "\n",
      "None\n",
      "Iteration:90\n",
      "action: [0.0008262]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49501086065547\n",
      "\n",
      "None\n",
      "Iteration:91\n",
      "action: [1.2014922]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49486376989634\n",
      "\n",
      "None\n",
      "Iteration:92\n",
      "action: [1.2842513]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.4943029972911\n",
      "\n",
      "None\n",
      "Iteration:93\n",
      "action: [-1.6597654]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49384190530517\n",
      "\n",
      "None\n",
      "Iteration:94\n",
      "action: [-1.6058704]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.4943704599701\n",
      "\n",
      "None\n",
      "Iteration:95\n",
      "action: [-1.9509906]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49500081983396\n",
      "\n",
      "None\n",
      "Iteration:96\n",
      "action: [-0.95460504]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.4956300155446\n",
      "\n",
      "None\n",
      "Iteration:97\n",
      "action: [-0.13188861]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49628846060486\n",
      "\n",
      "None\n",
      "Iteration:98\n",
      "action: [-0.14515756]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.4970569763519\n",
      "\n",
      "None\n",
      "Iteration:99\n",
      "action: [-0.0311763]\n",
      "goal: [0.03692, 0, 0.973]\n",
      "state: [0.073, 0.0, 1.083]\n",
      "reward: -146.49757380217315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action, _state = model.predict(obs, deterministic=False)\n",
    "    print(_state)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    applied_action = env._action_mapping_torque(action)\n",
    "    \n",
    "    print(f\"Iteration:{i}\")\n",
    "    print(f\"action: {action}\")\n",
    "    print(f\"goal: {goal[0]['position'][7]}\")\n",
    "    print(f\"state: {[round(ob,3) for ob in obs]}\")\n",
    "    print(f\"reward: {reward}\\n\")\n",
    "    \n",
    "    sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing observation_space and action_space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces \n",
    "_action_space = spaces.Box(0, 2, (2,2), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([[0. 0.]\n",
       " [0. 0.]], [[2. 2.]\n",
       " [2. 2.]], (2, 2), float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8621da41d1f75996b3114ed85820b1f2b9a1702c3804dac4519b835c2698d1e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('sc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
