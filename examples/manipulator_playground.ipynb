{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safe_control_gym.envs.manipulators.manipulator import BaseManipulator\n",
    "import numpy as np \n",
    "import random \n",
    "from time import sleep\n",
    "from safe_control_gym.envs.benchmark_env import Cost, Task\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO, DDPG, A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "Version = 4.1 Metal - 76.3\n",
      "Vendor = Apple\n",
      "Renderer = Apple M1 Pro\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    }
   ],
   "source": [
    "urdf_path = \"../safe_control_gym/envs/manipulators/assets/franka_panda/panda.urdf\"\n",
    "controlled_variable = \"torque\"\n",
    "target_space = \"joint\"\n",
    "\n",
    "controlled_joint_indices = [6]\n",
    "observed_link_indices = [7]\n",
    "observed_link_state_keys = [\"position\"]\n",
    "\n",
    "\n",
    "# orientation_goal = [0.603, 0.3687, -0.3697, 0.6026]\n",
    "# position_goal = [0.03692, 0, 0.973]\n",
    "\n",
    "# position_goal = [-0.127, 0.0, 1.012] # for 1 joint \n",
    "# -0.03458691, -0.01447242,  1.118461\n",
    "# position_goal = [0.0212 , -0.0288,  1.113] # 2joints\n",
    "\n",
    "goal = [0.0212, 1.113]\n",
    "# position_goal = [0.0868, -0.022,  1.014] # 2 joints\n",
    "# position_goal = [0.0, 0, 1]\n",
    "\n",
    "#TODO reuse this for future \n",
    "# goal = [{\n",
    "#     \"position\": [None for i in range(13)]\n",
    "# }]\n",
    "# goal[0][\"position\"][observed_link_indices[0]] = position_goal\n",
    "\n",
    "goal_type = \"point\"\n",
    "connection = \"gui\"\n",
    "control_method = \"classical\"\n",
    "dimensions = 2\n",
    "\n",
    "if dimensions == 2:\n",
    "    position_goal = [0.0212, 1.113]\n",
    "    \n",
    "elif dimensions == 3:\n",
    "    position_goal = [0.0212, 0, 1.113]\n",
    "    \n",
    "env = BaseManipulator(\n",
    "    urdf_path,\n",
    "    controlled_variable,\n",
    "    control_method,\n",
    "    target_space,\n",
    "    controlled_joint_indices = controlled_joint_indices,\n",
    "    observed_link_indices = observed_link_indices, \n",
    "    observed_link_state_keys = observed_link_state_keys,\n",
    "    goal = goal,\n",
    "    goal_type = goal_type,\n",
    "    cost = Cost.RL_REWARD,\n",
    "    connection = connection,\n",
    "    dimensions = dimensions\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: [0.088, -0.0, 0.953]\n",
      "obs: [0.09, 0.0, 0.955]\n",
      "obs: [0.096, 0.0, 0.963]\n",
      "obs: [0.103, 0.0, 0.973]\n",
      "obs: [0.11, 0.0, 0.987]\n",
      "obs: [0.115, 0.0, 1.004]\n",
      "obs: [0.119, -0.0, 1.029]\n",
      "obs: [0.117, 0.0, 1.061]\n",
      "obs: [0.108, 0.0, 1.093]\n",
      "obs: [0.095, 0.0, 1.119]\n",
      "obs: [0.08, 0.0, 1.137]\n",
      "obs: [0.071, 0.0, 1.147]\n",
      "obs: [0.064, 0.0, 1.152]\n",
      "obs: [0.058, -0.0, 1.151]\n",
      "obs: [0.06, -0.0, 1.147]\n",
      "obs: [0.067, -0.0, 1.141]\n",
      "obs: [0.079, -0.0, 1.133]\n",
      "obs: [0.093, -0.0, 1.121]\n",
      "obs: [0.108, -0.0, 1.106]\n",
      "obs: [0.125, -0.0, 1.085]\n",
      "obs: [0.141, 0.0, 1.058]\n",
      "obs: [0.159, 0.0, 1.055]\n",
      "obs: [0.177, 0.0, 1.05]\n",
      "obs: [0.193, 0.0, 1.046]\n",
      "obs: [0.208, 0.0, 1.042]\n",
      "obs: [0.222, 0.0, 1.038]\n",
      "obs: [0.235, 0.0, 1.033]\n",
      "obs: [0.247, 0.0, 1.029]\n",
      "obs: [0.258, 0.0, 1.025]\n",
      "obs: [0.268, 0.0, 1.022]\n",
      "obs: [0.277, 0.0, 1.018]\n",
      "obs: [0.285, 0.0, 1.015]\n",
      "obs: [0.292, 0.0, 1.012]\n",
      "obs: [0.298, 0.0, 1.009]\n",
      "obs: [0.303, 0.0, 1.007]\n",
      "obs: [0.306, 0.0, 1.005]\n",
      "obs: [0.309, 0.0, 1.004]\n",
      "obs: [0.312, 0.0, 1.003]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n",
      "obs: [0.313, 0.0, 1.002]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action_space = [4, 3]\n",
    "# action_space = [-400]\n",
    "\n",
    "for i in range(50):\n",
    "#     action_index = env.n_joints-2\n",
    "    action_index = 6\n",
    "    # action_list = np.zeros(env.n_joints)\n",
    "    action_list = [random.choice(action_space) for i in env.controlled_joint_indices]    \n",
    "    joint_states=  env._pb_client.getJointState(env.robot,\n",
    "                                            jointIndex=6,\n",
    "                                )\n",
    "    # action_list = [200]\n",
    "    link_states =  env._pb_client.getLinkState(env.robot,\n",
    "                                            linkIndex=7,\n",
    "                                            computeLinkVelocity=True)[-1]\n",
    "\n",
    "    obs,reward, done,info = env.step(action_list)\n",
    "    # if i%5== 0 :\n",
    "    # print(action_list)\n",
    "    obs_position = [round(i,3) for i in obs[:3]]\n",
    "    \n",
    "    print(f\"obs: {obs_position}\") # link position\n",
    "    # print(f\"reward: {reward}\")\n",
    "  #   print(f\"angular velocity: {obs}\") # velocity\n",
    "    # print(f\"joint state: {joint_states}\") # link position\n",
    "    # print(f\"link state: {link_states}\") # link position\n",
    "    # print(f\"observation: {}\")\n",
    "    # print(f\"position : {link_states[]}\")\n",
    "    sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring link states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, (0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_states=  env._pb_client.getJointState(env.robot,\n",
    "                                            jointIndex=6,\n",
    "                                )\n",
    "joint_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, (0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_states=  env._pb_client.getJointState(env.robot,\n",
    "                                            jointIndex=7,\n",
    "                                )\n",
    "\n",
    "joint_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_states =  env._pb_client.getLinkState(env.robot,\n",
    "                                            linkIndex=7,\n",
    "                                            computeLinkVelocity=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only extract world positions\n",
    "link_states_dict = {\n",
    "    \"position\": link_states[4],\n",
    "    \"orientation\": link_states[5],\n",
    "    \"linear_vel\": link_states[6],\n",
    "    \"angular_vel\": link_states[7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_states_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces \n",
    "# env.observation_space = spaces.Box(-np.inf, np.inf, (14,), dtype=np.float32)\n",
    "\n",
    "# orientation use quaternion so 4 dim \n",
    "\n",
    "# action_space = [spaces.Discrete(1) for i in range(env.n_joints-1)]\n",
    "# action_space.append(spaces.Box(-400, 400, shape=(1,), dtype=np.float32))\n",
    "\n",
    "# env.action_space = spaces.Tuple(\n",
    "#     action_space\n",
    "# )\n",
    "\n",
    "# dim dict\n",
    "dim_dict = {\n",
    "    \"position\":3 + 3,\n",
    "    \"orientation\" : 4+4\n",
    "}\n",
    "env.action_space = spaces.Box(-4.5, 4.5, (len(env.controlled_joint_indices),), dtype=np.float32)\n",
    "\n",
    "# TODO expand to multiple state keys \n",
    "env.observation_space = spaces.Box(-np.inf, np.inf, (dim_dict[observed_link_state_keys[0]], ), dtype=np.float32)\n",
    "\n",
    "# env.observation_space = spaces.Dict({\n",
    "#       'position': spaces.Box(-np.inf, np.inf, (env.n_joints, 3), dtype=np.float32),\n",
    "#       'orientation': spaces.Box(-np.inf, np.inf, (env.n_joints, 4), dtype=np.float32),\n",
    "#       'linear_vel': spaces.Box(-np.inf, np.inf, (env.n_joints, 3), dtype=np.float32),\n",
    "#       'angular_vel': spaces.Box(-np.inf, np.inf, (env.n_joints, 3), dtype=np.float32),\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09856006,  0.04260633,  1.12294292,  0.0212    , -0.0288    ,\n",
       "        1.113     ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 72   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 28   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000029?line=3'>4</a>\u001b[0m \u001b[39m# use this to stop if too many timesteps cannot converge to done \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000029?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000029?line=5'>6</a>\u001b[0m     ppo_model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:304\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=290'>291</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=291'>292</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=292'>293</a>\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=300'>301</a>\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=301'>302</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=303'>304</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=304'>305</a>\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=305'>306</a>\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=306'>307</a>\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=307'>308</a>\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=308'>309</a>\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=309'>310</a>\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=310'>311</a>\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=311'>312</a>\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=312'>313</a>\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py?line=313'>314</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=245'>246</a>\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=247'>248</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=249'>250</a>\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=251'>252</a>\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=252'>253</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=174'>175</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=175'>176</a>\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=177'>178</a>\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=179'>180</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=181'>182</a>\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=154'>155</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=155'>156</a>\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=156'>157</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=157'>158</a>\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=158'>159</a>\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=159'>160</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=160'>161</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=161'>162</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=42'>43</a>\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=43'>44</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=44'>45</a>\u001b[0m         )\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=45'>46</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=46'>47</a>\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=47'>48</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=88'>89</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=89'>90</a>\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=90'>91</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/stable_baselines3/common/monitor.py?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py:199\u001b[0m, in \u001b[0;36mBaseManipulator.step\u001b[0;34m(self, action_list)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=195'>196</a>\u001b[0m             applied_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_mapping_torque(action) \n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=196'>197</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_joint_apply_action(joint_index, applied_action)\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=198'>199</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pb_client\u001b[39m.\u001b[39;49mstepSimulation()\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=200'>201</a>\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_observation()\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=201'>202</a>\u001b[0m \u001b[39m# reward = random.choice(self.reward_space)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env.reset()\n",
    "ppo_model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# use this to stop if too many timesteps cannot converge to done \n",
    "for i in range(10):\n",
    "    ppo_model.learn(total_timesteps=10000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 76   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 26   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 82            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 49            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020561198 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -1.86e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.06e+06      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000109     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.45e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004539536 |\n",
      "|    clip_fraction        | 0.00713     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -4.53e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.41e+06    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 1.23e+07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011540849 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 1.01e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.16e+07     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 2.32e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 116          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011647319 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 4.05e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.83e+06     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 1.73e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 76   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 26   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 83            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 48            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028135072 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.82         |\n",
      "|    explained_variance   | -5.25e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.82e+06      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000291     |\n",
      "|    std                  | 0.991         |\n",
      "|    value_loss           | 7.69e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015082029 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -5.96e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.95e+06     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 1.04e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 6.41e+03     |\n",
      "|    ep_rew_mean          | -1.12e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049453564 |\n",
      "|    clip_fraction        | 0.00991      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | -3.58e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.12e+06     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00482     |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 1.09e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 6.41e+03     |\n",
      "|    ep_rew_mean          | -1.12e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 120          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016469882 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | -3.58e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.85e+06     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000806    |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 3.77e+06     |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 77   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 26   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004359697 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.16e+06     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000824    |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 7.81e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.42e+03     |\n",
      "|    ep_rew_mean          | -9.36e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004761602 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 4.77e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.5e+06      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000542    |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 1.1e+07      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.42e+03      |\n",
      "|    ep_rew_mean          | -9.36e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 83            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 98            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042603994 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.79         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.57e+06      |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.000319     |\n",
      "|    std                  | 0.978         |\n",
      "|    value_loss           | 8.08e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.42e+03     |\n",
      "|    ep_rew_mean          | -9.36e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015701682 |\n",
      "|    clip_fraction        | 0.00205      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -9.54e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.22e+06     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 7.28e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 284       |\n",
      "|    ep_rew_mean     | -3.27e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 58        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 34        |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 284          |\n",
      "|    ep_rew_mean          | -3.27e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049498654 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.11e+06     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 5.59e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 284           |\n",
      "|    ep_rew_mean          | -3.27e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 79            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 77            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034744976 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.8          |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7e+06         |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.00035      |\n",
      "|    std                  | 0.978         |\n",
      "|    value_loss           | 1.47e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 284         |\n",
      "|    ep_rew_mean          | -3.27e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003091102 |\n",
      "|    clip_fraction        | 0.0019      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.52e+06    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 7.11e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 284          |\n",
      "|    ep_rew_mean          | -3.27e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014063405 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.14e+06     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 1.74e+07     |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1.76e+03  |\n",
      "|    ep_rew_mean     | -2.64e+05 |\n",
      "| time/              |           |\n",
      "|    fps             | 68        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 29        |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.76e+03    |\n",
      "|    ep_rew_mean          | -2.64e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002305442 |\n",
      "|    clip_fraction        | 0.00381     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.87e+06    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 6.62e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.76e+03    |\n",
      "|    ep_rew_mean          | -2.64e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002597095 |\n",
      "|    clip_fraction        | 0.00293     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.13e+06    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 6.19e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.76e+03     |\n",
      "|    ep_rew_mean          | -2.64e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050295163 |\n",
      "|    clip_fraction        | 0.00767      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.32e+06     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00406     |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 7.7e+06      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 4.18e+03     |\n",
      "|    ep_rew_mean          | -6.41e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054546986 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.94e+06     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00563     |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 7.9e+06      |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 80   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 25   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 88           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052606463 |\n",
      "|    clip_fraction        | 0.0176       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.21e+06     |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 6.36e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011625574 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.04e+06     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 9.13e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 92            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 88            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018698548 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.82         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.23e+07      |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.000296     |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 4.64e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 92            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 110           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056808017 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.82         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.63e+07      |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.00128      |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 7.37e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 79   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 25   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026047616 |\n",
      "|    clip_fraction        | 0.00254      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.25e+06     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 4.49e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014636468 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.95e+06     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 1.97e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 86            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 94            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069414324 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.82         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.07e+06      |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.00094      |\n",
      "|    std                  | 0.988         |\n",
      "|    value_loss           | 1.53e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 86            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 117           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024689664 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.81         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.52e+07      |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.000479     |\n",
      "|    std                  | 0.986         |\n",
      "|    value_loss           | 4.84e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 76   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 26   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005238912 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.3e+06      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000378    |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 7.88e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.194901e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.25e+06     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -2.49e-05    |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 6.82e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004899179 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.4e+06      |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000611    |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 4.64e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 86            |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 118           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035524156 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.79         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.54e+06      |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.00029      |\n",
      "|    std                  | 0.976         |\n",
      "|    value_loss           | 1.36e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 76   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 26   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001964785 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.07e+06     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000108    |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 7.52e+06     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 72         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00275524 |\n",
      "|    clip_fraction        | 0.00313    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.79      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.87e+06   |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.00286   |\n",
      "|    std                  | 0.978      |\n",
      "|    value_loss           | 1.43e+07   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022437884 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.42e+06     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00264     |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 1.89e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013536997 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.42e+07     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 2.79e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 79   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 25   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 774          |\n",
      "|    ep_rew_mean          | -1.08e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 63           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040752823 |\n",
      "|    clip_fraction        | 0.00605      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.09e+06     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    std                  | 0.969        |\n",
      "|    value_loss           | 7.06e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 774          |\n",
      "|    ep_rew_mean          | -1.08e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007134364 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.32e+06     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00031     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 6.18e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 774          |\n",
      "|    ep_rew_mean          | -1.08e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 77           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019884657 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.61e+06     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    std                  | 0.964        |\n",
      "|    value_loss           | 1.46e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 774          |\n",
      "|    ep_rew_mean          | -1.08e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010107313 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.72e+06     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00089     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 1.58e+07     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# env.reset()\n",
    "ppo_model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# use this to stop if too many timesteps cannot converge to done \n",
    "for i in range(10):\n",
    "    ppo_model.learn(total_timesteps=10000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model.save(\"manipulator_100000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0\n",
      "action: [-0.5811313   0.56101274]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.088, 0.0, 1.033, 0.021, -0.029, 1.113]\n",
      "reward: -175.60000715289036\n",
      "\n",
      "Iteration:5\n",
      "action: [ 0.05037165 -0.66742194]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.088, -0.003, 1.029, 0.021, -0.029, 1.113]\n",
      "reward: -176.4528595138341\n",
      "\n",
      "Iteration:10\n",
      "action: [0.11259472 0.725588  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.087, 0.002, 1.027, 0.021, -0.029, 1.113]\n",
      "reward: -183.29054464064538\n",
      "\n",
      "Iteration:15\n",
      "action: [-0.3309157   0.06978023]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.088, 0.0, 1.035, 0.021, -0.029, 1.113]\n",
      "reward: -173.11900618895888\n",
      "\n",
      "Iteration:20\n",
      "action: [ 0.7653686 -1.6649845]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.084, -0.017, 1.05, 0.021, -0.029, 1.113]\n",
      "reward: -138.6604439228773\n",
      "\n",
      "Iteration:25\n",
      "action: [-0.7841464   0.49291724]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.087, -0.009, 1.038, 0.021, -0.029, 1.113]\n",
      "reward: -161.35404070317745\n",
      "\n",
      "Iteration:30\n",
      "action: [ 0.35791013 -0.30590567]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.087, 0.005, 1.041, 0.021, -0.029, 1.113]\n",
      "reward: -171.55869489014148\n",
      "\n",
      "Iteration:35\n",
      "action: [-0.68730104 -0.5888699 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.086, 0.004, 1.029, 0.021, -0.029, 1.113]\n",
      "reward: -182.12639305219054\n",
      "\n",
      "Iteration:40\n",
      "action: [-0.40042698  0.14112523]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.085, -0.019, 1.03, 0.021, -0.029, 1.113]\n",
      "reward: -156.47732836604118\n",
      "\n",
      "Iteration:45\n",
      "action: [-1.3949986  -0.53482705]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.067, -0.057, 1.033, 0.021, -0.029, 1.113]\n",
      "reward: -154.6074588894844\n",
      "\n",
      "Iteration:50\n",
      "action: [ 0.3837418  -0.92795384]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.068, -0.056, 1.029, 0.021, -0.029, 1.113]\n",
      "reward: -158.22002935409546\n",
      "\n",
      "Iteration:55\n",
      "action: [1.2528273  0.62541914]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.074, -0.048, 1.026, 0.021, -0.029, 1.113]\n",
      "reward: -158.54938599467278\n",
      "\n",
      "Iteration:60\n",
      "action: [-0.00416465  1.0059314 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.08, -0.038, 1.039, 0.021, -0.029, 1.113]\n",
      "reward: -141.6045864224434\n",
      "\n",
      "Iteration:65\n",
      "action: [-1.3807725   0.42056364]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.059, -0.062, 1.054, 0.021, -0.029, 1.113]\n",
      "reward: -130.9723471403122\n",
      "\n",
      "Iteration:70\n",
      "action: [-1.5826261   0.86691594]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.038, -0.071, 1.068, 0.021, -0.029, 1.113]\n",
      "reward: -105.26541146636008\n",
      "\n",
      "Iteration:75\n",
      "action: [0.39044222 0.881027  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.036, -0.061, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -75.36956194043158\n",
      "\n",
      "Iteration:80\n",
      "action: [ 1.3749019 -0.1311377]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.034, -0.055, 1.093, 0.021, -0.029, 1.113]\n",
      "reward: -59.226094663143144\n",
      "\n",
      "Iteration:85\n",
      "action: [-0.05096376 -0.3617623 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.042, -0.064, 1.077, 0.021, -0.029, 1.113]\n",
      "reward: -92.17856723070143\n",
      "\n",
      "Iteration:90\n",
      "action: [ 0.37426442 -0.14284085]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.033, -0.071, 1.073, 0.021, -0.029, 1.113]\n",
      "reward: -93.65856918692587\n",
      "\n",
      "Iteration:95\n",
      "action: [-1.1251101  -0.47687915]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.033, -0.073, 1.07, 0.021, -0.029, 1.113]\n",
      "reward: -98.77202931046484\n",
      "\n",
      "Iteration:100\n",
      "action: [-0.0660474 -1.0734422]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.024, -0.076, 1.07, 0.021, -0.029, 1.113]\n",
      "reward: -93.12578189373015\n",
      "\n",
      "Iteration:105\n",
      "action: [ 0.40258995 -1.0751028 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.011, -0.079, 1.07, 0.021, -0.029, 1.113]\n",
      "reward: -103.05135999768972\n",
      "\n",
      "Iteration:110\n",
      "action: [-0.9211276   0.15343651]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.014, -0.081, 1.063, 0.021, -0.029, 1.113]\n",
      "reward: -109.4961867198348\n",
      "\n",
      "Iteration:115\n",
      "action: [-0.04342263 -1.0776031 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.007, -0.084, 1.059, 0.021, -0.029, 1.113]\n",
      "reward: -123.68294619917869\n",
      "\n",
      "Iteration:120\n",
      "action: [-0.08217984  1.3107342 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.009, -0.084, 1.057, 0.021, -0.029, 1.113]\n",
      "reward: -141.01584651470185\n",
      "\n",
      "Iteration:125\n",
      "action: [0.83693445 0.41066638]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.023, -0.084, 1.054, 0.021, -0.029, 1.113]\n",
      "reward: -158.75077457129956\n",
      "\n",
      "Iteration:130\n",
      "action: [ 1.5680931 -0.4511341]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, -0.087, 1.047, 0.021, -0.029, 1.113]\n",
      "reward: -151.93340671211482\n",
      "\n",
      "Iteration:135\n",
      "action: [-0.5892934 -1.1461989]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, -0.085, 1.051, 0.021, -0.029, 1.113]\n",
      "reward: -134.59523511081935\n",
      "\n",
      "Iteration:140\n",
      "action: [ 0.15307015 -0.33028626]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.016, -0.087, 1.039, 0.021, -0.029, 1.113]\n",
      "reward: -169.5323383897543\n",
      "\n",
      "Iteration:145\n",
      "action: [1.3381923  0.24188189]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.028, -0.085, 1.026, 0.021, -0.029, 1.113]\n",
      "reward: -192.72419274747372\n",
      "\n",
      "Iteration:150\n",
      "action: [0.2222507  0.65329295]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, -0.087, 1.047, 0.021, -0.029, 1.113]\n",
      "reward: -150.8757260158658\n",
      "\n",
      "Iteration:155\n",
      "action: [ 0.9449047 -1.0716922]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.023, -0.081, 1.053, 0.021, -0.029, 1.113]\n",
      "reward: -115.24646255373953\n",
      "\n",
      "Iteration:160\n",
      "action: [-0.7739073   0.38340163]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.044, -0.072, 1.053, 0.021, -0.029, 1.113]\n",
      "reward: -125.21655383706093\n",
      "\n",
      "Iteration:165\n",
      "action: [ 0.11046368 -0.48924515]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.036, -0.072, 1.065, 0.021, -0.029, 1.113]\n",
      "reward: -105.71936044096945\n",
      "\n",
      "Iteration:170\n",
      "action: [-1.0544009  -0.69775367]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.037, -0.066, 1.075, 0.021, -0.029, 1.113]\n",
      "reward: -91.30144390463828\n",
      "\n",
      "Iteration:175\n",
      "action: [ 0.7832073 -1.000714 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.014, -0.071, 1.082, 0.021, -0.029, 1.113]\n",
      "reward: -79.59208101928233\n",
      "\n",
      "Iteration:180\n",
      "action: [-1.0053704 -1.0938038]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.018, -0.073, 1.075, 0.021, -0.029, 1.113]\n",
      "reward: -85.37815534472465\n",
      "\n",
      "Iteration:185\n",
      "action: [-1.0010381  2.3454664]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.005, -0.083, 1.061, 0.021, -0.029, 1.113]\n",
      "reward: -133.20007477849722\n",
      "\n",
      "Iteration:190\n",
      "action: [0.3112894  0.48372743]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.053, -0.065, 1.069, 0.021, -0.029, 1.113]\n",
      "reward: -154.5863563656807\n",
      "\n",
      "Iteration:195\n",
      "action: [-0.8624278  1.2033471]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.065, -0.052, 1.079, 0.021, -0.029, 1.113]\n",
      "reward: -143.05202313661576\n",
      "\n",
      "Iteration:200\n",
      "action: [-1.6125695   0.04001485]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.075, -0.049, 1.072, 0.021, -0.029, 1.113]\n",
      "reward: -157.566685461998\n",
      "\n",
      "Iteration:205\n",
      "action: [-0.29555684  0.26243055]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.082, -0.038, 1.076, 0.021, -0.029, 1.113]\n",
      "reward: -150.0874126791954\n",
      "\n",
      "Iteration:210\n",
      "action: [ 0.09799332 -1.1751767 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.088, -0.038, 1.065, 0.021, -0.029, 1.113]\n",
      "reward: -166.38327785730363\n",
      "\n",
      "Iteration:215\n",
      "action: [-1.6019468 -0.5008979]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, -0.047, 1.06, 0.021, -0.029, 1.113]\n",
      "reward: -176.29662179350854\n",
      "\n",
      "Iteration:220\n",
      "action: [-1.1636319  0.7602729]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.086, -0.05, 1.045, 0.021, -0.029, 1.113]\n",
      "reward: -196.30890362858773\n",
      "\n",
      "Iteration:225\n",
      "action: [-0.9613196  0.9194228]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.087, -0.047, 1.045, 0.021, -0.029, 1.113]\n",
      "reward: -194.49402787685395\n",
      "\n",
      "Iteration:230\n",
      "action: [-0.24416365 -0.75854075]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.087, -0.028, 1.074, 0.021, -0.029, 1.113]\n",
      "reward: -148.10076105594632\n",
      "\n",
      "Iteration:235\n",
      "action: [-0.2937475   0.31307003]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, -0.025, 1.081, 0.021, -0.029, 1.113]\n",
      "reward: -140.84987464547154\n",
      "\n",
      "Iteration:240\n",
      "action: [ 1.2630221 -1.4149182]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.076, -0.02, 1.094, 0.021, -0.029, 1.113]\n",
      "reward: -125.61266432702539\n",
      "\n",
      "Iteration:245\n",
      "action: [ 1.6008693 -0.5930701]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.074, -0.015, 1.096, 0.021, -0.029, 1.113]\n",
      "reward: -126.26998341828582\n",
      "\n",
      "Iteration:250\n",
      "action: [0.3537184 1.551174 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, -0.02, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -142.67501543462274\n",
      "\n",
      "Iteration:255\n",
      "action: [-0.44606096  1.8594288 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.081, -0.023, 1.087, 0.021, -0.029, 1.113]\n",
      "reward: -133.5268610268831\n",
      "\n",
      "Iteration:260\n",
      "action: [0.30814257 0.0749537 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.075, -0.022, 1.094, 0.021, -0.029, 1.113]\n",
      "reward: -122.96432006359097\n",
      "\n",
      "Iteration:265\n",
      "action: [-1.2601796  0.2550758]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.059, -0.019, 1.107, 0.021, -0.029, 1.113]\n",
      "reward: -96.2738686352968\n",
      "\n",
      "Iteration:270\n",
      "action: [ 0.78324944 -0.02499876]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.051, -0.015, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -87.71713200211522\n",
      "\n",
      "Iteration:275\n",
      "action: [-0.8334094   0.17080855]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.053, -0.016, 1.111, 0.021, -0.029, 1.113]\n",
      "reward: -89.4017194509506\n",
      "\n",
      "Iteration:280\n",
      "action: [1.6263623  0.29981506]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.047, -0.012, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -86.21946451067924\n",
      "\n",
      "Iteration:285\n",
      "action: [-1.6608716  1.7818576]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.034, -0.014, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -74.28436827659607\n",
      "\n",
      "Iteration:290\n",
      "action: [-0.2868384  0.3279577]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, 0.008, 1.12, 0.021, -0.029, 1.113]\n",
      "reward: -71.41547751426698\n",
      "\n",
      "Iteration:295\n",
      "action: [-0.1256223   0.71481496]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.014, 0.011, 1.119, 0.021, -0.029, 1.113]\n",
      "reward: -52.86070433259012\n",
      "\n",
      "Iteration:300\n",
      "action: [ 0.4171732 -1.5843881]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.022, 0.012, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -46.082457295060166\n",
      "\n",
      "Iteration:305\n",
      "action: [-0.13931465 -0.0941793 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.003, -0.001, 1.122, 0.021, -0.029, 1.113]\n",
      "reward: -60.766921460628524\n",
      "\n",
      "Iteration:310\n",
      "action: [0.8591285  0.08878026]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.056, -0.03, 1.099, 0.021, -0.029, 1.113]\n",
      "reward: -92.7720109462738\n",
      "\n",
      "Iteration:315\n",
      "action: [-1.1197628   0.25163198]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.067, -0.032, 1.097, 0.021, -0.029, 1.113]\n",
      "reward: -107.25206159949302\n",
      "\n",
      "Iteration:320\n",
      "action: [-0.52188414  0.04893143]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.053, -0.018, 1.11, 0.021, -0.029, 1.113]\n",
      "reward: -88.08959956467149\n",
      "\n",
      "Iteration:325\n",
      "action: [-1.4486293 -1.6032197]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.038, -0.007, 1.119, 0.021, -0.029, 1.113]\n",
      "reward: -86.31982383131981\n",
      "\n",
      "Iteration:330\n",
      "action: [ 0.2537876 -1.2720797]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.049, -0.009, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -91.01633828878403\n",
      "\n",
      "Iteration:335\n",
      "action: [0.09201305 1.3963778 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.053, -0.01, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -93.71441352367398\n",
      "\n",
      "Iteration:340\n",
      "action: [-0.54515475 -0.2191718 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.017, -0.0, 1.121, 0.021, -0.029, 1.113]\n",
      "reward: -74.94150529429317\n",
      "\n",
      "Iteration:345\n",
      "action: [-0.8759368 -0.3213405]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.002, 0.002, 1.121, 0.021, -0.029, 1.113]\n",
      "reward: -62.623887404799476\n",
      "\n",
      "Iteration:350\n",
      "action: [-0.93866163  0.27214703]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.004, 1.12, 0.021, -0.029, 1.113]\n",
      "reward: -50.898928277194514\n",
      "\n",
      "Iteration:355\n",
      "action: [-0.3836242 -1.0254202]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.036, 0.008, 1.111, 0.021, -0.029, 1.113]\n",
      "reward: -53.034426620602595\n",
      "\n",
      "Iteration:360\n",
      "action: [2.0909781  0.85074484]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.047, 0.005, 1.107, 0.021, -0.029, 1.113]\n",
      "reward: -65.97763809785245\n",
      "\n",
      "Iteration:365\n",
      "action: [ 0.59228444 -0.5879522 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.055, 0.014, 1.1, 0.021, -0.029, 1.113]\n",
      "reward: -88.69740923345087\n",
      "\n",
      "Iteration:370\n",
      "action: [ 0.7899672 -0.3793675]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.053, 0.02, 1.1, 0.021, -0.029, 1.113]\n",
      "reward: -93.8635375112295\n",
      "\n",
      "Iteration:375\n",
      "action: [-1.5278119  -0.03507122]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.042, 0.022, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -76.38465983271597\n",
      "\n",
      "Iteration:380\n",
      "action: [ 0.1103671 -1.5100865]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.034, 0.011, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -56.45257211327554\n",
      "\n",
      "Iteration:385\n",
      "action: [-1.0595181  0.9075238]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.006, 0.004, 1.121, 0.021, -0.029, 1.113]\n",
      "reward: -55.810571730136886\n",
      "\n",
      "Iteration:390\n",
      "action: [-1.454617   1.2474309]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.003, 1.121, 0.021, -0.029, 1.113]\n",
      "reward: -51.08122442662717\n",
      "\n",
      "Iteration:395\n",
      "action: [-0.43428123  0.02761391]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.014, 0.003, 1.12, 0.021, -0.029, 1.113]\n",
      "reward: -45.271415345370784\n",
      "\n",
      "Iteration:400\n",
      "action: [ 0.21838444 -0.3377288 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.023, 0.005, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -40.39340163618327\n",
      "\n",
      "Iteration:405\n",
      "action: [-0.609966  -0.5097287]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.01, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -48.087607836723336\n",
      "\n",
      "Iteration:410\n",
      "action: [-0.48445296  0.7541805 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.035, 0.016, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -58.8802341133356\n",
      "\n",
      "Iteration:415\n",
      "action: [ 1.6908387 -1.0485039]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.048, 0.034, 1.086, 0.021, -0.029, 1.113]\n",
      "reward: -116.78987366557119\n",
      "\n",
      "Iteration:420\n",
      "action: [0.4905384  0.10568321]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.061, 0.024, 1.089, 0.021, -0.029, 1.113]\n",
      "reward: -116.15617570877073\n",
      "\n",
      "Iteration:425\n",
      "action: [-0.34243065 -0.05059759]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.069, 0.027, 1.068, 0.021, -0.029, 1.113]\n",
      "reward: -149.11628884673118\n",
      "\n",
      "Iteration:430\n",
      "action: [-1.8631598   0.77302605]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.068, 0.029, 1.073, 0.021, -0.029, 1.113]\n",
      "reward: -143.37467399835586\n",
      "\n",
      "Iteration:435\n",
      "action: [ 1.5595481 -0.6289411]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.069, 0.018, 1.078, 0.021, -0.029, 1.113]\n",
      "reward: -129.5272055774927\n",
      "\n",
      "Iteration:440\n",
      "action: [ 1.5001557 -0.665598 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.06, 0.018, 1.091, 0.021, -0.029, 1.113]\n",
      "reward: -108.67182535529135\n",
      "\n",
      "Iteration:445\n",
      "action: [-1.1603502 -1.1110337]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.041, 0.02, 1.107, 0.021, -0.029, 1.113]\n",
      "reward: -73.69597067534922\n",
      "\n",
      "Iteration:450\n",
      "action: [-0.12805475 -0.8229934 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.039, 0.017, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -70.08482513427732\n",
      "\n",
      "Iteration:455\n",
      "action: [-0.47857165 -1.5001426 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.034, 0.008, 1.111, 0.021, -0.029, 1.113]\n",
      "reward: -51.86702401787041\n",
      "\n",
      "Iteration:460\n",
      "action: [-0.06842336  0.6421564 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.019, 0.001, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -37.377331255003824\n",
      "\n",
      "Iteration:465\n",
      "action: [-1.2138133   0.27141446]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.007, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -50.68863936513664\n",
      "\n",
      "Iteration:470\n",
      "action: [0.19533205 0.87802094]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.016, 0.004, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -38.04641714133324\n",
      "\n",
      "Iteration:475\n",
      "action: [ 2.2642412  -0.24199906]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.012, 1.102, 0.021, -0.029, 1.113]\n",
      "reward: -58.499074517190444\n",
      "\n",
      "Iteration:480\n",
      "action: [-1.2419049  0.6631873]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.027, 0.016, 1.105, 0.021, -0.029, 1.113]\n",
      "reward: -58.34198554456233\n",
      "\n",
      "Iteration:485\n",
      "action: [-0.24357177 -2.0697036 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.037, 0.01, 1.1, 0.021, -0.029, 1.113]\n",
      "reward: -68.0579421803355\n",
      "\n",
      "Iteration:490\n",
      "action: [0.43485105 1.2929642 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.019, 0.007, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -41.541053064167514\n",
      "\n",
      "Iteration:495\n",
      "action: [-0.16210997  1.4243728 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, 0.027, 1.108, 0.021, -0.029, 1.113]\n",
      "reward: -88.77123641967772\n",
      "\n",
      "Iteration:500\n",
      "action: [-0.6544398  -0.02550028]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.01, 0.027, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -94.04988895356654\n",
      "\n",
      "Iteration:505\n",
      "action: [-0.16342698  0.34277326]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, 0.016, 1.108, 0.021, -0.029, 1.113]\n",
      "reward: -76.6536344885826\n",
      "\n",
      "Iteration:510\n",
      "action: [-0.5304684 -1.0446006]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.002, 0.006, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -61.654398508369916\n",
      "\n",
      "Iteration:515\n",
      "action: [-0.91873574 -0.55495083]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.013, 0.007, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -70.89844354242086\n",
      "\n",
      "Iteration:520\n",
      "action: [0.2206081  0.44078112]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.022, 0.004, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -79.67266588285565\n",
      "\n",
      "Iteration:525\n",
      "action: [2.6327267  0.15703939]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.021, 0.018, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -90.107959985733\n",
      "\n",
      "Iteration:530\n",
      "action: [ 1.4040926 -1.1212882]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.024, 0.036, 1.104, 0.021, -0.029, 1.113]\n",
      "reward: -119.74201101064679\n",
      "\n",
      "Iteration:535\n",
      "action: [-0.8702731   0.41402817]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.035, 0.031, 1.111, 0.021, -0.029, 1.113]\n",
      "reward: -118.75653016567227\n",
      "\n",
      "Iteration:540\n",
      "action: [0.85497904 0.59060395]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.031, 0.02, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -101.44649381935594\n",
      "\n",
      "Iteration:545\n",
      "action: [ 1.0235587  -0.35168445]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.032, 0.032, 1.107, 0.021, -0.029, 1.113]\n",
      "reward: -120.51515403389928\n",
      "\n",
      "Iteration:550\n",
      "action: [0.25748727 0.5130273 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.036, 0.029, 1.11, 0.021, -0.029, 1.113]\n",
      "reward: -118.25193586945531\n",
      "\n",
      "Iteration:555\n",
      "action: [-1.9484476  -0.46070814]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.049, 0.017, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -119.5621984899044\n",
      "\n",
      "Iteration:560\n",
      "action: [-1.568644   1.1876944]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.056, 0.011, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -122.02577021718025\n",
      "\n",
      "Iteration:565\n",
      "action: [-2.5143344  -0.21976276]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.051, 0.022, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -124.81288340687752\n",
      "\n",
      "Iteration:570\n",
      "action: [0.16665341 2.5479398 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.05, 0.019, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -122.38779392838478\n",
      "\n",
      "Iteration:575\n",
      "action: [-0.46001372  0.60056394]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.046, 0.032, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -128.77116474509236\n",
      "\n",
      "Iteration:580\n",
      "action: [-0.6570995 -0.9483414]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.048, 0.035, 1.111, 0.021, -0.029, 1.113]\n",
      "reward: -134.3855572044849\n",
      "\n",
      "Iteration:585\n",
      "action: [-1.941121   -0.16684647]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.054, 0.023, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -129.7068494260311\n",
      "\n",
      "Iteration:590\n",
      "action: [0.16571225 0.8778291 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.054, 0.018, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -126.18510526418686\n",
      "\n",
      "Iteration:595\n",
      "action: [ 0.04280763 -0.85206205]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.055, 0.018, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -126.81795012950897\n",
      "\n",
      "Iteration:600\n",
      "action: [-0.9441183  0.3668359]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.05, 0.015, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -119.28565901517868\n",
      "\n",
      "Iteration:605\n",
      "action: [-0.1487379 -1.3423603]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.035, 0.035, 1.1, 0.021, -0.029, 1.113]\n",
      "reward: -133.31020373106\n",
      "\n",
      "Iteration:610\n",
      "action: [1.4455438  0.55858165]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.032, 0.032, 1.103, 0.021, -0.029, 1.113]\n",
      "reward: -123.39086207747457\n",
      "\n",
      "Iteration:615\n",
      "action: [-0.04587768  0.8725997 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.037, 0.032, 1.105, 0.021, -0.029, 1.113]\n",
      "reward: -127.38366174697873\n",
      "\n",
      "Iteration:620\n",
      "action: [-0.64777386  0.3425302 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.026, 0.03, 1.101, 0.021, -0.029, 1.113]\n",
      "reward: -117.51837405562398\n",
      "\n",
      "Iteration:625\n",
      "action: [-0.95874834  0.3962346 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.028, 0.033, 1.1, 0.021, -0.029, 1.113]\n",
      "reward: -124.66396886110303\n",
      "\n",
      "Iteration:630\n",
      "action: [ 0.8264455 -1.4927092]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.021, 0.025, 1.101, 0.021, -0.029, 1.113]\n",
      "reward: -108.53540833294389\n",
      "\n",
      "Iteration:635\n",
      "action: [-0.01789708  0.5110713 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.026, 0.027, 1.103, 0.021, -0.029, 1.113]\n",
      "reward: -113.28296671807763\n",
      "\n",
      "Iteration:640\n",
      "action: [-0.13742276  1.8771222 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.017, 0.034, 1.092, 0.021, -0.029, 1.113]\n",
      "reward: -121.46900343894956\n",
      "\n",
      "Iteration:645\n",
      "action: [-0.8930223  -0.78994703]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.022, 0.048, 1.092, 0.021, -0.029, 1.113]\n",
      "reward: -140.96761244535443\n",
      "\n",
      "Iteration:650\n",
      "action: [-0.15916488 -0.32092205]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.029, 0.038, 1.102, 0.021, -0.029, 1.113]\n",
      "reward: -127.53252434730527\n",
      "\n",
      "Iteration:655\n",
      "action: [-1.6314642  1.5447003]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.036, 0.026, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -115.50856503844258\n",
      "\n",
      "Iteration:660\n",
      "action: [-0.9230129   0.24342795]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.016, 0.04, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -135.05052524805066\n",
      "\n",
      "Iteration:665\n",
      "action: [0.26792657 0.81941235]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.017, 0.03, 1.06, 0.021, -0.029, 1.113]\n",
      "reward: -115.48212851583956\n",
      "\n",
      "Iteration:670\n",
      "action: [ 8.2638115e-04 -1.3693479e+00]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.028, 1.041, 0.021, -0.029, 1.113]\n",
      "reward: -135.29455286860465\n",
      "\n",
      "Iteration:675\n",
      "action: [ 0.02785907 -0.9648173 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.027, 0.037, 1.051, 0.021, -0.029, 1.113]\n",
      "reward: -132.72729699909686\n",
      "\n",
      "Iteration:680\n",
      "action: [ 0.20152697 -0.4640407 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.017, 0.056, 1.039, 0.021, -0.029, 1.113]\n",
      "reward: -162.3289094418287\n",
      "\n",
      "Iteration:685\n",
      "action: [0.29877365 0.38037917]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.009, 0.066, 1.033, 0.021, -0.029, 1.113]\n",
      "reward: -186.85944593697783\n",
      "\n",
      "Iteration:690\n",
      "action: [ 0.84273446 -1.9800532 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.003, 0.071, 1.04, 0.021, -0.029, 1.113]\n",
      "reward: -190.74827717803416\n",
      "\n",
      "Iteration:695\n",
      "action: [ 0.16319235 -0.8370298 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.008, 0.063, 1.047, 0.021, -0.029, 1.113]\n",
      "reward: -170.71863199770448\n",
      "\n",
      "Iteration:700\n",
      "action: [-1.7202218  0.5061171]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.066, 1.047, 0.021, -0.029, 1.113]\n",
      "reward: -178.4884112998843\n",
      "\n",
      "Iteration:705\n",
      "action: [-0.63683516 -0.7404735 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.003, 0.066, 1.051, 0.021, -0.029, 1.113]\n",
      "reward: -174.8751125410199\n",
      "\n",
      "Iteration:710\n",
      "action: [0.6840875 0.506381 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.0, 0.067, 1.056, 0.021, -0.029, 1.113]\n",
      "reward: -173.50827279686925\n",
      "\n",
      "Iteration:715\n",
      "action: [-0.97803056 -0.8779822 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.005, 0.074, 1.043, 0.021, -0.029, 1.113]\n",
      "reward: -198.36842111498115\n",
      "\n",
      "Iteration:720\n",
      "action: [-0.09150101  0.46592775]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.009, 0.051, 1.072, 0.021, -0.029, 1.113]\n",
      "reward: -133.57924233376977\n",
      "\n",
      "Iteration:725\n",
      "action: [-0.3384182   0.88482517]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.041, 1.077, 0.021, -0.029, 1.113]\n",
      "reward: -117.15789633989331\n",
      "\n",
      "Iteration:730\n",
      "action: [-0.18037933  1.2510153 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.013, 0.025, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -90.60888673365115\n",
      "\n",
      "Iteration:735\n",
      "action: [-0.25995338  1.013132  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.022, 0.018, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -92.88132374584673\n",
      "\n",
      "Iteration:740\n",
      "action: [0.72527266 0.03460235]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.034, 0.012, 1.047, 0.021, -0.029, 1.113]\n",
      "reward: -119.35615909695623\n",
      "\n",
      "Iteration:745\n",
      "action: [-0.94612324  1.1428164 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.031, 0.041, 1.034, 0.021, -0.029, 1.113]\n",
      "reward: -158.45357996821403\n",
      "\n",
      "Iteration:750\n",
      "action: [-0.39817056  0.5858951 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.037, 0.029, 1.022, 0.021, -0.029, 1.113]\n",
      "reward: -164.30026104152202\n",
      "\n",
      "Iteration:755\n",
      "action: [1.4410832 1.0297064]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.022, 0.022, 0.982, 0.021, -0.029, 1.113]\n",
      "reward: -183.1836532562971\n",
      "\n",
      "Iteration:760\n",
      "action: [-2.1963122  -0.67044306]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.017, 0.021, 0.972, 0.021, -0.029, 1.113]\n",
      "reward: -194.45333379507062\n",
      "\n",
      "Iteration:765\n",
      "action: [-0.85015315  0.64607847]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.021, 0.025, 0.978, 0.021, -0.029, 1.113]\n",
      "reward: -188.4267957061529\n",
      "\n",
      "Iteration:770\n",
      "action: [-0.40185255  0.9897891 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.023, 0.028, 0.983, 0.021, -0.029, 1.113]\n",
      "reward: -189.0635117650032\n",
      "\n",
      "Iteration:775\n",
      "action: [-1.5532948   0.14125313]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.026, 0.02, 0.983, 0.021, -0.029, 1.113]\n",
      "reward: -183.56273320913314\n",
      "\n",
      "Iteration:780\n",
      "action: [ 0.3543206  -0.14306562]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.022, 0.986, 0.021, -0.029, 1.113]\n",
      "reward: -183.90950185656547\n",
      "\n",
      "Iteration:785\n",
      "action: [0.87450254 0.32269165]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.033, 0.025, 0.997, 0.021, -0.029, 1.113]\n",
      "reward: -181.50106815099716\n",
      "\n",
      "Iteration:790\n",
      "action: [-1.0151223  0.9036008]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.029, 0.03, 0.993, 0.021, -0.029, 1.113]\n",
      "reward: -186.33635526001453\n",
      "\n",
      "Iteration:795\n",
      "action: [-0.25504953 -0.39103156]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.033, 0.028, 1.0, 0.021, -0.029, 1.113]\n",
      "reward: -181.95223433375358\n",
      "\n",
      "Iteration:800\n",
      "action: [-0.945985   -0.44948548]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.037, 0.021, 1.006, 0.021, -0.029, 1.113]\n",
      "reward: -172.975391125679\n",
      "\n",
      "Iteration:805\n",
      "action: [-1.3195829 -0.3895452]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.022, 0.988, 0.021, -0.029, 1.113]\n",
      "reward: -182.92284754514694\n",
      "\n",
      "Iteration:810\n",
      "action: [0.6402725 2.0883775]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.025, 0.002, 0.977, 0.021, -0.029, 1.113]\n",
      "reward: -170.612718886137\n",
      "\n",
      "Iteration:815\n",
      "action: [-0.7392979  1.6859969]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.029, 0.004, 0.982, 0.021, -0.029, 1.113]\n",
      "reward: -171.52464108020067\n",
      "\n",
      "Iteration:820\n",
      "action: [-2.3281853 -0.6120032]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.005, 0.981, 0.021, -0.029, 1.113]\n",
      "reward: -172.91852876022458\n",
      "\n",
      "Iteration:825\n",
      "action: [-0.85799307 -0.69020206]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.007, 0.981, 0.021, -0.029, 1.113]\n",
      "reward: -174.30764760300517\n",
      "\n",
      "Iteration:830\n",
      "action: [1.3551267  0.70578694]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.032, 0.007, 0.988, 0.021, -0.029, 1.113]\n",
      "reward: -172.61190151274204\n",
      "\n",
      "Iteration:835\n",
      "action: [-0.11968924 -0.09741242]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.031, 0.014, 0.987, 0.021, -0.029, 1.113]\n",
      "reward: -178.391193947196\n",
      "\n",
      "Iteration:840\n",
      "action: [ 0.12302355 -0.19751333]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.045, 0.013, 1.034, 0.021, -0.029, 1.113]\n",
      "reward: -145.22620753794908\n",
      "\n",
      "Iteration:845\n",
      "action: [-1.3393798   0.03160144]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.026, 0.013, 1.064, 0.021, -0.029, 1.113]\n",
      "reward: -95.60319965183733\n",
      "\n",
      "Iteration:850\n",
      "action: [-0.50284934 -0.19815527]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.02, 0.005, 1.052, 0.021, -0.029, 1.113]\n",
      "reward: -95.85201350226997\n",
      "\n",
      "Iteration:855\n",
      "action: [0.95287204 0.8330409 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.007, 1.068, 0.021, -0.029, 1.113]\n",
      "reward: -90.21776710823177\n",
      "\n",
      "Iteration:860\n",
      "action: [0.14983489 0.6416682 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.017, 1.072, 0.021, -0.029, 1.113]\n",
      "reward: -103.26605039834975\n",
      "\n",
      "Iteration:865\n",
      "action: [0.9727624  0.53724647]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.017, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -109.52284681797026\n",
      "\n",
      "Iteration:870\n",
      "action: [-0.6447952   0.51499254]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.007, 1.072, 0.021, -0.029, 1.113]\n",
      "reward: -93.6799555197358\n",
      "\n",
      "Iteration:875\n",
      "action: [-0.58924896  0.03201488]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.011, 0.015, 1.091, 0.021, -0.029, 1.113]\n",
      "reward: -99.08979322016236\n",
      "\n",
      "Iteration:880\n",
      "action: [-0.01307543 -0.00792624]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.019, 0.017, 1.09, 0.021, -0.029, 1.113]\n",
      "reward: -109.07699476182458\n",
      "\n",
      "Iteration:885\n",
      "action: [ 0.3782653 -0.6958797]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.035, 0.007, 1.108, 0.021, -0.029, 1.113]\n",
      "reward: -95.96197056770322\n",
      "\n",
      "Iteration:890\n",
      "action: [ 1.2322367 -1.141143 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.047, 0.003, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -101.40324671566486\n",
      "\n",
      "Iteration:895\n",
      "action: [0.9298472 1.7548842]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.086, 0.0, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -139.39678910002112\n",
      "\n",
      "Iteration:900\n",
      "action: [0.9468908  0.34273267]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, 0.002, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -138.8129276111722\n",
      "\n",
      "Iteration:905\n",
      "action: [0.5600046  0.00685733]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.057, 0.006, 1.111, 0.021, -0.029, 1.113]\n",
      "reward: -115.40620523691175\n",
      "\n",
      "Iteration:910\n",
      "action: [-0.0416222  -0.36685154]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.019, 0.023, 1.073, 0.021, -0.029, 1.113]\n",
      "reward: -132.09492462873456\n",
      "\n",
      "Iteration:915\n",
      "action: [2.05054   1.2156303]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.002, 0.018, 1.084, 0.021, -0.029, 1.113]\n",
      "reward: -98.53321249783038\n",
      "\n",
      "Iteration:920\n",
      "action: [-0.3463111   0.77220976]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.0, 0.024, 1.08, 0.021, -0.029, 1.113]\n",
      "reward: -107.36160475015639\n",
      "\n",
      "Iteration:925\n",
      "action: [-0.2024259  0.6860898]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.001, 0.016, 1.09, 0.021, -0.029, 1.113]\n",
      "reward: -90.57428251206873\n",
      "\n",
      "Iteration:930\n",
      "action: [ 1.8941513  -0.57341367]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.009, 0.006, 1.079, 0.021, -0.029, 1.113]\n",
      "reward: -81.4749349951744\n",
      "\n",
      "Iteration:935\n",
      "action: [1.0286674 2.153309 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.006, 0.012, 1.083, 0.021, -0.029, 1.113]\n",
      "reward: -87.0177199244499\n",
      "\n",
      "Iteration:940\n",
      "action: [-1.4331028  1.4417316]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.003, 0.023, 1.078, 0.021, -0.029, 1.113]\n",
      "reward: -105.37466901540755\n",
      "\n",
      "Iteration:945\n",
      "action: [1.8178381  0.10227339]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.022, 1.074, 0.021, -0.029, 1.113]\n",
      "reward: -106.0035315454006\n",
      "\n",
      "Iteration:950\n",
      "action: [ 0.44281006 -0.44309533]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.028, 1.054, 0.021, -0.029, 1.113]\n",
      "reward: -124.74218505620955\n",
      "\n",
      "Iteration:955\n",
      "action: [-0.13863626  0.40103307]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.016, 0.022, 1.061, 0.021, -0.029, 1.113]\n",
      "reward: -108.27405127882956\n",
      "\n",
      "Iteration:960\n",
      "action: [-0.24599543 -1.2875721 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.014, 0.02, 1.065, 0.021, -0.029, 1.113]\n",
      "reward: -103.41942246258257\n",
      "\n",
      "Iteration:965\n",
      "action: [ 0.01190089 -1.7069099 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.009, 0.008, 1.076, 0.021, -0.029, 1.113]\n",
      "reward: -84.81814089417456\n",
      "\n",
      "Iteration:970\n",
      "action: [-1.1389346 -0.5953101]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.009, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -81.88718307018279\n",
      "\n",
      "Iteration:975\n",
      "action: [-0.2987573  1.4404109]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.008, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -81.21090131998061\n",
      "\n",
      "Iteration:980\n",
      "action: [-0.49229524  0.84066105]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.013, 0.006, 1.082, 0.021, -0.029, 1.113]\n",
      "reward: -74.44207213073967\n",
      "\n",
      "Iteration:985\n",
      "action: [1.5763435 0.6861873]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.013, 0.006, 1.083, 0.021, -0.029, 1.113]\n",
      "reward: -73.11251788586377\n",
      "\n",
      "Iteration:990\n",
      "action: [ 0.00994036 -0.08188353]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.014, 0.009, 1.081, 0.021, -0.029, 1.113]\n",
      "reward: -77.19431846588849\n",
      "\n",
      "Iteration:995\n",
      "action: [0.91339993 0.8817786 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.009, 1.091, 0.021, -0.029, 1.113]\n",
      "reward: -75.74945531040429\n",
      "\n",
      "Iteration:1000\n",
      "action: [-0.38162282 -1.0135199 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.011, 0.013, 1.072, 0.021, -0.029, 1.113]\n",
      "reward: -92.77889254689215\n",
      "\n",
      "Iteration:1005\n",
      "action: [-0.9191953   0.46783137]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.009, 0.017, 1.08, 0.021, -0.029, 1.113]\n",
      "reward: -91.26328054070471\n",
      "\n",
      "Iteration:1010\n",
      "action: [0.26383072 1.3882197 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.006, 0.025, 1.079, 0.021, -0.029, 1.113]\n",
      "reward: -102.29309025406836\n",
      "\n",
      "Iteration:1015\n",
      "action: [0.32394224 0.42399308]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.001, 0.029, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -105.97559745609759\n",
      "\n",
      "Iteration:1020\n",
      "action: [0.01700249 0.9908835 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.002, 0.038, 1.073, 0.021, -0.029, 1.113]\n",
      "reward: -126.6476971209049\n",
      "\n",
      "Iteration:1025\n",
      "action: [0.22040007 0.32433546]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.001, 0.055, 1.059, 0.021, -0.029, 1.113]\n",
      "reward: -160.0624595880508\n",
      "\n",
      "Iteration:1030\n",
      "action: [ 0.66822034 -0.8975747 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.001, 0.061, 1.036, 0.021, -0.029, 1.113]\n",
      "reward: -186.05668080225584\n",
      "\n",
      "Iteration:1035\n",
      "action: [0.30046546 0.9944594 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.014, 0.041, 1.049, 0.021, -0.029, 1.113]\n",
      "reward: -142.18990626931188\n",
      "\n",
      "Iteration:1040\n",
      "action: [0.73401654 0.747787  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.025, 0.019, 1.034, 0.021, -0.029, 1.113]\n",
      "reward: -130.30400869846343\n",
      "\n",
      "Iteration:1045\n",
      "action: [-0.09011148 -0.7314701 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.005, 1.035, 0.021, -0.029, 1.113]\n",
      "reward: -119.62205541729925\n",
      "\n",
      "Iteration:1050\n",
      "action: [-0.2990804  1.8481073]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.028, 0.006, 1.041, 0.021, -0.029, 1.113]\n",
      "reward: -113.56877719461916\n",
      "\n",
      "Iteration:1055\n",
      "action: [-0.09004377 -0.27239966]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.024, 0.014, 1.049, 0.021, -0.029, 1.113]\n",
      "reward: -109.67339200377462\n",
      "\n",
      "Iteration:1060\n",
      "action: [ 0.23832819 -0.51744115]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.023, 0.013, 1.049, 0.021, -0.029, 1.113]\n",
      "reward: -107.5037978351116\n",
      "\n",
      "Iteration:1065\n",
      "action: [-0.7181277  0.7879483]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.02, 0.026, 1.045, 0.021, -0.029, 1.113]\n",
      "reward: -123.54134890437125\n",
      "\n",
      "Iteration:1070\n",
      "action: [0.2576658 1.7546612]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.022, 0.021, 1.043, 0.021, -0.029, 1.113]\n",
      "reward: -120.92542273402212\n",
      "\n",
      "Iteration:1075\n",
      "action: [-2.706022  2.192466]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.021, 0.028, 1.041, 0.021, -0.029, 1.113]\n",
      "reward: -128.72434715926644\n",
      "\n",
      "Iteration:1080\n",
      "action: [0.52443707 0.7414763 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.017, 0.041, 1.027, 0.021, -0.029, 1.113]\n",
      "reward: -158.87227217853066\n",
      "\n",
      "Iteration:1085\n",
      "action: [-1.1229559  -0.18667394]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.053, 1.025, 0.021, -0.029, 1.113]\n",
      "reward: -180.65236165374515\n",
      "\n",
      "Iteration:1090\n",
      "action: [-0.38248557 -0.8297609 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.008, 0.048, 1.046, 0.021, -0.029, 1.113]\n",
      "reward: -157.46949176490304\n",
      "\n",
      "Iteration:1095\n",
      "action: [-0.74016774 -0.53726053]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.018, 0.03, 1.04, 0.021, -0.029, 1.113]\n",
      "reward: -135.0872566103935\n",
      "\n",
      "Iteration:1100\n",
      "action: [-0.43092018 -0.5269964 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.023, 0.016, 1.036, 0.021, -0.029, 1.113]\n",
      "reward: -123.37844630181787\n",
      "\n",
      "Iteration:1105\n",
      "action: [-0.9136288 -1.5742879]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.021, 0.018, 1.05, 0.021, -0.029, 1.113]\n",
      "reward: -110.63109460473059\n",
      "\n",
      "Iteration:1110\n",
      "action: [-1.2294735  -0.53389436]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.017, 0.013, 1.056, 0.021, -0.029, 1.113]\n",
      "reward: -103.9868363291025\n",
      "\n",
      "Iteration:1115\n",
      "action: [2.1949632  0.35521978]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.019, 0.014, 1.04, 0.021, -0.029, 1.113]\n",
      "reward: -118.25007974356411\n",
      "\n",
      "Iteration:1120\n",
      "action: [-0.29415095  0.29821286]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.038, 1.004, 0.021, -0.029, 1.113]\n",
      "reward: -185.194592282176\n",
      "\n",
      "Iteration:1125\n",
      "action: [1.0949044  0.52237517]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.016, 0.041, 1.009, 0.021, -0.029, 1.113]\n",
      "reward: -179.69444143772122\n",
      "\n",
      "Iteration:1130\n",
      "action: [-0.302349  1.219385]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.015, 0.043, 1.013, 0.021, -0.029, 1.113]\n",
      "reward: -177.8371076211333\n",
      "\n",
      "Iteration:1135\n",
      "action: [ 0.15075013 -1.1806409 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.022, 0.029, 1.017, 0.021, -0.029, 1.113]\n",
      "reward: -155.34235907196998\n",
      "\n",
      "Iteration:1140\n",
      "action: [ 0.07787544 -0.429703  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.018, 0.024, 0.998, 0.021, -0.029, 1.113]\n",
      "reward: -171.2317944616079\n",
      "\n",
      "Iteration:1145\n",
      "action: [-2.3763373  0.9669786]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.022, 0.022, 1.001, 0.021, -0.029, 1.113]\n",
      "reward: -162.67181185483932\n",
      "\n",
      "Iteration:1150\n",
      "action: [ 0.2956551 -0.6976627]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.021, 0.984, 0.021, -0.029, 1.113]\n",
      "reward: -188.51830932497975\n",
      "\n",
      "Iteration:1155\n",
      "action: [-1.3339717  1.0178652]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.029, 0.986, 0.021, -0.029, 1.113]\n",
      "reward: -194.0602635145187\n",
      "\n",
      "Iteration:1160\n",
      "action: [-0.15856227  0.25713423]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.007, 0.028, 0.98, 0.021, -0.029, 1.113]\n",
      "reward: -204.36852547526357\n",
      "\n",
      "Iteration:1165\n",
      "action: [-0.37593478  1.1164764 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.002, 0.039, 0.981, 0.021, -0.029, 1.113]\n",
      "reward: -219.42700567841527\n",
      "\n",
      "Iteration:1170\n",
      "action: [-0.04742427  2.3179698 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.048, 0.991, 0.021, -0.029, 1.113]\n",
      "reward: -216.24943177402017\n",
      "\n",
      "Iteration:1175\n",
      "action: [0.2272399  0.50004715]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.002, 0.051, 0.991, 0.021, -0.029, 1.113]\n",
      "reward: -221.04264359176156\n",
      "\n",
      "Iteration:1180\n",
      "action: [-2.19633     0.78165513]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, 0.055, 0.987, 0.021, -0.029, 1.113]\n",
      "reward: -237.66646470129487\n",
      "\n",
      "Iteration:1185\n",
      "action: [0.20584685 1.4123023 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.002, 0.052, 0.987, 0.021, -0.029, 1.113]\n",
      "reward: -230.18842871487138\n",
      "\n",
      "Iteration:1190\n",
      "action: [-0.40347618 -0.742435  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.001, 0.058, 0.997, 0.021, -0.029, 1.113]\n",
      "reward: -224.7471523582935\n",
      "\n",
      "Iteration:1195\n",
      "action: [1.9578209  0.83981913]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.002, 0.057, 0.996, 0.021, -0.029, 1.113]\n",
      "reward: -226.44184465706346\n",
      "\n",
      "Iteration:1200\n",
      "action: [-0.33209053 -0.8878839 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.005, 0.057, 0.992, 0.021, -0.029, 1.113]\n",
      "reward: -233.11019997298715\n",
      "\n",
      "Iteration:1205\n",
      "action: [ 1.002389  -1.1050055]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.06, 1.016, 0.021, -0.029, 1.113]\n",
      "reward: -201.5501831769943\n",
      "\n",
      "Iteration:1210\n",
      "action: [ 0.37868536 -0.29791552]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.009, 0.058, 1.033, 0.021, -0.029, 1.113]\n",
      "reward: -178.85531234741208\n",
      "\n",
      "Iteration:1215\n",
      "action: [2.2671888 1.3889089]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.002, 0.05, 1.08, 0.021, -0.029, 1.113]\n",
      "reward: -135.52737358212468\n",
      "\n",
      "Iteration:1220\n",
      "action: [-0.5607196 -1.1991198]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.008, 0.047, 1.077, 0.021, -0.029, 1.113]\n",
      "reward: -142.1348464787006\n",
      "\n",
      "Iteration:1225\n",
      "action: [ 1.0736479 -1.774458 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.001, 0.032, 1.078, 0.021, -0.029, 1.113]\n",
      "reward: -116.8929983973503\n",
      "\n",
      "Iteration:1230\n",
      "action: [-0.2818043  -0.08743519]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.003, 0.031, 1.081, 0.021, -0.029, 1.113]\n",
      "reward: -116.06990668177603\n",
      "\n",
      "Iteration:1235\n",
      "action: [-0.06276219  0.47286734]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.005, 0.017, 1.093, 0.021, -0.029, 1.113]\n",
      "reward: -92.19110134243964\n",
      "\n",
      "Iteration:1240\n",
      "action: [-0.0875989  -0.86837775]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.018, 0.014, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -89.26705579459664\n",
      "\n",
      "Iteration:1245\n",
      "action: [-0.502813   -0.31190565]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.073, 0.002, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -129.96611472964287\n",
      "\n",
      "Iteration:1250\n",
      "action: [ 1.7552793  -0.68542975]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.1, -0.001, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -150.53507600724697\n",
      "\n",
      "Iteration:1255\n",
      "action: [-0.9423045  -0.24768911]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.128, -0.01, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -175.87695765495297\n",
      "\n",
      "Iteration:1260\n",
      "action: [-1.1521294 -0.8350305]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.135, -0.007, 1.102, 0.021, -0.029, 1.113]\n",
      "reward: -188.8483476936817\n",
      "\n",
      "Iteration:1265\n",
      "action: [0.48810107 0.20826273]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.136, -0.0, 1.102, 0.021, -0.029, 1.113]\n",
      "reward: -195.95227978378531\n",
      "\n",
      "Iteration:1270\n",
      "action: [0.00264574 0.08324262]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.143, -0.005, 1.097, 0.021, -0.029, 1.113]\n",
      "reward: -204.09557438641784\n",
      "\n",
      "Iteration:1275\n",
      "action: [0.11739779 0.03032154]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.136, -0.005, 1.102, 0.021, -0.029, 1.113]\n",
      "reward: -191.94635312259194\n",
      "\n",
      "Iteration:1280\n",
      "action: [-1.1362138 -0.7307292]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.109, 0.0, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -159.35704213380814\n",
      "\n",
      "Iteration:1285\n",
      "action: [0.37668797 0.34011638]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.099, 0.0, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -151.26465176045895\n",
      "\n",
      "Iteration:1290\n",
      "action: [-0.9559477  -0.14633445]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, 0.002, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -140.10312742367387\n",
      "\n",
      "Iteration:1295\n",
      "action: [-0.85938513 -0.8609266 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.08, 0.002, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -136.1254456806928\n",
      "\n",
      "Iteration:1300\n",
      "action: [-0.8126852  -0.15269388]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.085, 0.002, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -140.8817279972136\n",
      "\n",
      "Iteration:1305\n",
      "action: [ 0.9544979  -0.21909438]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.091, 0.001, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -146.406531881541\n",
      "\n",
      "Iteration:1310\n",
      "action: [-0.56485087 -0.33200574]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.106, 0.0, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -156.52582895755768\n",
      "\n",
      "Iteration:1315\n",
      "action: [-1.5519665  -0.27022117]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.112, 0.0, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -162.43759012967345\n",
      "\n",
      "Iteration:1320\n",
      "action: [-0.11987239 -0.9107007 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.107, 0.001, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -158.99588362127542\n",
      "\n",
      "Iteration:1325\n",
      "action: [ 1.9646192 -1.414776 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.101, 0.001, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -154.54064597189426\n",
      "\n",
      "Iteration:1330\n",
      "action: [0.1839311 0.8321665]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.115, -0.008, 1.111, 0.021, -0.029, 1.113]\n",
      "reward: -159.09431624412534\n",
      "\n",
      "Iteration:1335\n",
      "action: [ 0.71324795 -1.3707951 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.111, -0.008, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -153.4251657426357\n",
      "\n",
      "Iteration:1340\n",
      "action: [ 0.17064838 -0.45740128]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.106, -0.009, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -147.43844521045685\n",
      "\n",
      "Iteration:1345\n",
      "action: [-1.1698023  -0.39734071]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.097, -0.024, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -123.75521409511563\n",
      "\n",
      "Iteration:1350\n",
      "action: [1.4727178  0.51385975]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.104, -0.035, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -137.8794128537178\n",
      "\n",
      "Iteration:1355\n",
      "action: [-0.06465137 -0.40706864]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.102, -0.028, 1.11, 0.021, -0.029, 1.113]\n",
      "reward: -126.77316862344739\n",
      "\n",
      "Iteration:1360\n",
      "action: [0.7816456  0.31018028]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.102, -0.036, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -136.39611401557923\n",
      "\n",
      "Iteration:1365\n",
      "action: [-0.40763116  0.44377813]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.098, -0.029, 1.11, 0.021, -0.029, 1.113]\n",
      "reward: -122.7925757408142\n",
      "\n",
      "Iteration:1370\n",
      "action: [ 0.06337208 -0.37684083]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.091, -0.003, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -141.53517138957977\n",
      "\n",
      "Iteration:1375\n",
      "action: [-0.8979598  -0.97319984]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.09, 0.002, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -145.00599455833435\n",
      "\n",
      "Iteration:1380\n",
      "action: [ 0.25127906 -1.3508539 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.09, -0.0, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -143.43805712461472\n",
      "\n",
      "Iteration:1385\n",
      "action: [-1.0919547  1.1568487]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.102, -0.025, 1.11, 0.021, -0.029, 1.113]\n",
      "reward: -128.90756624937055\n",
      "\n",
      "Iteration:1390\n",
      "action: [-1.5509737  0.6738739]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.092, 0.001, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -146.23567563295364\n",
      "\n",
      "Iteration:1395\n",
      "action: [0.10042358 2.0109458 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.076, 0.013, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -142.81422746181488\n",
      "\n",
      "Iteration:1400\n",
      "action: [ 0.5039836 -1.3271492]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.07, 0.027, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -146.62093770503998\n",
      "\n",
      "Iteration:1405\n",
      "action: [-0.40476194 -0.07507706]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.067, 0.038, 1.108, 0.021, -0.029, 1.113]\n",
      "reward: -160.54927307367322\n",
      "\n",
      "Iteration:1410\n",
      "action: [-0.11824012  1.0360526 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.058, 0.02, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -130.49681213498116\n",
      "\n",
      "Iteration:1415\n",
      "action: [0.0423824  0.05898103]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.057, 0.008, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -119.75472335517406\n",
      "\n",
      "Iteration:1420\n",
      "action: [-1.4067625   0.51289344]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.07, 0.006, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -130.32470186054707\n",
      "\n",
      "Iteration:1425\n",
      "action: [ 0.33487916 -1.1696324 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.067, 0.006, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -127.3372760862112\n",
      "\n",
      "Iteration:1430\n",
      "action: [-0.89882976 -0.18794987]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.07, 0.004, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -128.84779934585094\n",
      "\n",
      "Iteration:1435\n",
      "action: [-0.3590314 -1.0262767]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.083, -0.0, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -137.4691492766142\n",
      "\n",
      "Iteration:1440\n",
      "action: [ 0.4701606  -0.88826567]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, 0.0, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -138.55242830514908\n",
      "\n",
      "Iteration:1445\n",
      "action: [-0.44152972  0.35899073]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.086, -0.0, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -140.10249086469412\n",
      "\n",
      "Iteration:1450\n",
      "action: [-0.4439952  -0.70589906]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.091, -0.0, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -143.58729318529367\n",
      "\n",
      "Iteration:1455\n",
      "action: [0.7786421  0.96724874]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, 0.001, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -138.53692085295916\n",
      "\n",
      "Iteration:1460\n",
      "action: [ 1.1407874 -1.1638699]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.06, 0.005, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -117.96557818353176\n",
      "\n",
      "Iteration:1465\n",
      "action: [-0.24116665 -0.13727216]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.06, 0.006, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -119.51543212682009\n",
      "\n",
      "Iteration:1470\n",
      "action: [0.99806905 0.8971129 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.063, 0.001, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -118.27403454855084\n",
      "\n",
      "Iteration:1475\n",
      "action: [ 2.3224006 -0.4691272]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.061, 0.003, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -118.10316572710872\n",
      "\n",
      "Iteration:1480\n",
      "action: [0.38884288 0.03537823]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.074, 0.001, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -129.93697271496058\n",
      "\n",
      "Iteration:1485\n",
      "action: [-0.88533264 -0.36822847]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.089, 0.0, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -142.4847339168191\n",
      "\n",
      "Iteration:1490\n",
      "action: [-1.009661   -0.23138131]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.088, 0.0, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -141.95990104973316\n",
      "\n",
      "Iteration:1495\n",
      "action: [-0.67158866  0.73375344]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.092, 0.0, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -144.994787953794\n",
      "\n",
      "Iteration:1500\n",
      "action: [1.458319  1.3995985]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.096, -0.0, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -148.42613980919123\n",
      "\n",
      "Iteration:1505\n",
      "action: [ 1.7505167 -1.0186325]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.091, 0.001, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -144.59490877389908\n",
      "\n",
      "Iteration:1510\n",
      "action: [-0.24752906  0.46163008]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.098, -0.001, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -149.29562535881996\n",
      "\n",
      "Iteration:1515\n",
      "action: [-2.4833074 -0.2830342]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.095, 0.001, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -147.98783938214183\n",
      "\n",
      "Iteration:1520\n",
      "action: [0.09774353 1.506192  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.085, 0.001, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -139.75820000097156\n",
      "\n",
      "Iteration:1525\n",
      "action: [ 0.6752558 -1.247909 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.052, 0.012, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -114.48869955539703\n",
      "\n",
      "Iteration:1530\n",
      "action: [ 0.9255886 -0.3774735]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.059, -0.002, 1.12, 0.021, -0.029, 1.113]\n",
      "reward: -114.18652077019215\n",
      "\n",
      "Iteration:1535\n",
      "action: [ 0.405559  -1.0577044]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.075, -0.007, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -120.57169441878796\n",
      "\n",
      "Iteration:1540\n",
      "action: [-1.1557182 -1.1501222]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.086, -0.014, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -126.48994784057138\n",
      "\n",
      "Iteration:1545\n",
      "action: [-0.08267466 -1.6732882 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.087, -0.014, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -126.65842781960961\n",
      "\n",
      "Iteration:1550\n",
      "action: [1.3814121 0.9227227]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.073, -0.005, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -121.18788723647594\n",
      "\n",
      "Iteration:1555\n",
      "action: [0.7494029  0.59185696]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.02, 0.017, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -91.33081632852551\n",
      "\n",
      "Iteration:1560\n",
      "action: [-1.6697701 -0.6172798]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.02, 0.035, 1.052, 0.021, -0.029, 1.113]\n",
      "reward: -126.0031958222389\n",
      "\n",
      "Iteration:1565\n",
      "action: [1.6799071 0.1788981]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.053, 0.004, 1.039, 0.021, -0.029, 1.113]\n",
      "reward: -137.95194004140794\n",
      "\n",
      "Iteration:1570\n",
      "action: [-0.93481344  0.63057476]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.057, 0.009, 1.023, 0.021, -0.029, 1.113]\n",
      "reward: -163.59951437860727\n",
      "\n",
      "Iteration:1575\n",
      "action: [0.66512835 0.41811568]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.054, 0.02, 1.018, 0.021, -0.029, 1.113]\n",
      "reward: -177.611641561985\n",
      "\n",
      "Iteration:1580\n",
      "action: [-0.00809047 -0.08950362]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.05, 0.036, 1.016, 0.021, -0.029, 1.113]\n",
      "reward: -190.33398849368095\n",
      "\n",
      "Iteration:1585\n",
      "action: [ 0.16328952 -0.2162297 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.057, 0.01, 1.031, 0.021, -0.029, 1.113]\n",
      "reward: -156.50100072324275\n",
      "\n",
      "Iteration:1590\n",
      "action: [-0.57326084 -0.01923377]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.055, 0.019, 1.044, 0.021, -0.029, 1.113]\n",
      "reward: -150.7332790404558\n",
      "\n",
      "Iteration:1595\n",
      "action: [ 1.2929169 -0.7886345]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.054, 0.009, 1.046, 0.021, -0.029, 1.113]\n",
      "reward: -137.18686343580484\n",
      "\n",
      "Iteration:1600\n",
      "action: [0.47727898 1.7055162 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.053, 0.009, 1.055, 0.021, -0.029, 1.113]\n",
      "reward: -127.85037977099417\n",
      "\n",
      "Iteration:1605\n",
      "action: [ 0.29238665 -1.398114  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.041, 0.031, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -125.25913489460943\n",
      "\n",
      "Iteration:1610\n",
      "action: [1.0592613 0.5331187]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.04, 0.029, 1.07, 0.021, -0.029, 1.113]\n",
      "reward: -119.15439737439154\n",
      "\n",
      "Iteration:1615\n",
      "action: [ 0.5208958  -0.97906715]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.041, 0.025, 1.071, 0.021, -0.029, 1.113]\n",
      "reward: -115.43989924192427\n",
      "\n",
      "Iteration:1620\n",
      "action: [-0.23765016  0.0099922 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.039, 0.031, 1.071, 0.021, -0.029, 1.113]\n",
      "reward: -119.80248612761496\n",
      "\n",
      "Iteration:1625\n",
      "action: [-0.07707729  0.3439315 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.045, 0.022, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -120.84332851171492\n",
      "\n",
      "Iteration:1630\n",
      "action: [ 0.16582096 -0.29367903]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.045, 0.026, 1.063, 0.021, -0.029, 1.113]\n",
      "reward: -128.78411633372306\n",
      "\n",
      "Iteration:1635\n",
      "action: [-0.52231276  0.84419835]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.043, 0.031, 1.063, 0.021, -0.029, 1.113]\n",
      "reward: -131.14123044013976\n",
      "\n",
      "Iteration:1640\n",
      "action: [-1.5624326 -1.1977125]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.048, 0.026, 1.064, 0.021, -0.029, 1.113]\n",
      "reward: -131.1022732168436\n",
      "\n",
      "Iteration:1645\n",
      "action: [0.20504168 0.07211158]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.033, 0.034, 1.082, 0.021, -0.029, 1.113]\n",
      "reward: -105.56769741177557\n",
      "\n",
      "Iteration:1650\n",
      "action: [-0.765527   -0.14835082]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.03, 0.037, 1.07, 0.021, -0.029, 1.113]\n",
      "reward: -117.90971269309519\n",
      "\n",
      "Iteration:1655\n",
      "action: [ 0.02895489 -0.4512857 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.035, 0.022, 1.075, 0.021, -0.029, 1.113]\n",
      "reward: -103.16574198603628\n",
      "\n",
      "Iteration:1660\n",
      "action: [ 1.4660265  -0.05011955]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.014, 0.023, 1.094, 0.021, -0.029, 1.113]\n",
      "reward: -78.65904207527636\n",
      "\n",
      "Iteration:1665\n",
      "action: [-1.9213034   0.38169536]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.002, 0.025, 1.104, 0.021, -0.029, 1.113]\n",
      "reward: -86.16050641238688\n",
      "\n",
      "Iteration:1670\n",
      "action: [1.0738558  0.95143783]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.03, 1.095, 0.021, -0.029, 1.113]\n",
      "reward: -93.73955364525317\n",
      "\n",
      "Iteration:1675\n",
      "action: [ 0.573081  -1.2649537]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.032, 1.097, 0.021, -0.029, 1.113]\n",
      "reward: -93.9888072311878\n",
      "\n",
      "Iteration:1680\n",
      "action: [0.5444238 1.2936473]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.014, 0.031, 1.108, 0.021, -0.029, 1.113]\n",
      "reward: -99.09033524990079\n",
      "\n",
      "Iteration:1685\n",
      "action: [0.3164227 1.1545279]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.017, 0.034, 1.104, 0.021, -0.029, 1.113]\n",
      "reward: -110.68275320529935\n",
      "\n",
      "Iteration:1690\n",
      "action: [ 0.45590574 -1.3598421 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.017, 0.038, 1.101, 0.021, -0.029, 1.113]\n",
      "reward: -117.04657721519467\n",
      "\n",
      "Iteration:1695\n",
      "action: [-0.12594107  0.64849836]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.026, 0.046, 1.1, 0.021, -0.029, 1.113]\n",
      "reward: -135.65089675784108\n",
      "\n",
      "Iteration:1700\n",
      "action: [-0.07858878  1.0136453 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.008, 0.036, 1.097, 0.021, -0.029, 1.113]\n",
      "reward: -110.09680601954457\n",
      "\n",
      "Iteration:1705\n",
      "action: [-0.40314502 -0.63487023]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.005, 0.046, 1.083, 0.021, -0.029, 1.113]\n",
      "reward: -129.57898649573323\n",
      "\n",
      "Iteration:1710\n",
      "action: [-0.90877604 -0.12312415]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.02, 0.059, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -156.8528127968311\n",
      "\n",
      "Iteration:1715\n",
      "action: [-1.3839965  1.2006215]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.023, 0.045, 1.098, 0.021, -0.029, 1.113]\n",
      "reward: -132.93460133671758\n",
      "\n",
      "Iteration:1720\n",
      "action: [-0.9134354 -0.5150238]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.019, 0.035, 1.101, 0.021, -0.029, 1.113]\n",
      "reward: -115.22486183047292\n",
      "\n",
      "Iteration:1725\n",
      "action: [0.8450981 0.3979489]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.019, 0.031, 1.103, 0.021, -0.029, 1.113]\n",
      "reward: -110.36055657267568\n",
      "\n",
      "Iteration:1730\n",
      "action: [ 0.5021706  -0.18953133]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.023, 0.028, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -108.38378901779649\n",
      "\n",
      "Iteration:1735\n",
      "action: [ 0.0056849 -0.1876078]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.03, 0.028, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -111.88428226113317\n",
      "\n",
      "Iteration:1740\n",
      "action: [0.28061098 1.0717093 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.043, 0.019, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -115.18951979279518\n",
      "\n",
      "Iteration:1745\n",
      "action: [-0.8951917  1.1561792]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.059, 0.012, 1.119, 0.021, -0.029, 1.113]\n",
      "reward: -126.70486521720886\n",
      "\n",
      "Iteration:1750\n",
      "action: [0.7521422  0.12963048]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.061, 0.007, 1.119, 0.021, -0.029, 1.113]\n",
      "reward: -124.49520927667618\n",
      "\n",
      "Iteration:1755\n",
      "action: [-0.3821776  -0.15933722]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.061, 0.012, 1.119, 0.021, -0.029, 1.113]\n",
      "reward: -128.9267438352108\n",
      "\n",
      "Iteration:1760\n",
      "action: [0.46453905 0.76222026]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.074, -0.009, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -118.89806863665581\n",
      "\n",
      "Iteration:1765\n",
      "action: [-1.0269855   0.30440882]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.082, -0.01, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -125.1059817969799\n",
      "\n",
      "Iteration:1770\n",
      "action: [2.8276496 0.9261869]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.079, -0.003, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -130.98380629718304\n",
      "\n",
      "Iteration:1775\n",
      "action: [ 0.5653889 -1.123005 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.058, 0.006, 1.119, 0.021, -0.029, 1.113]\n",
      "reward: -120.06846916675568\n",
      "\n",
      "Iteration:1780\n",
      "action: [-0.19708924 -1.2434216 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.051, 0.008, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -113.7622446641326\n",
      "\n",
      "Iteration:1785\n",
      "action: [ 0.27456242 -1.0291328 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.035, 0.008, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -96.82171634584662\n",
      "\n",
      "Iteration:1790\n",
      "action: [-1.3523366   0.09514263]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.025, 0.006, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -87.95272681117055\n",
      "\n",
      "Iteration:1795\n",
      "action: [0.34781966 1.2565577 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.023, 0.015, 1.103, 0.021, -0.029, 1.113]\n",
      "reward: -98.55481452494857\n",
      "\n",
      "Iteration:1800\n",
      "action: [-0.13839519 -0.05156685]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.024, 0.019, 1.103, 0.021, -0.029, 1.113]\n",
      "reward: -104.43404997885224\n",
      "\n",
      "Iteration:1805\n",
      "action: [-0.54410243 -1.4529972 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.033, 0.011, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -95.20921978354451\n",
      "\n",
      "Iteration:1810\n",
      "action: [1.8753406 1.2015866]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.056, 0.005, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -114.88000218570232\n",
      "\n",
      "Iteration:1815\n",
      "action: [-1.5033557   0.66549003]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.059, 0.007, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -119.43740778416395\n",
      "\n",
      "Iteration:1820\n",
      "action: [-1.3287156  -0.69356185]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.056, 0.012, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -119.9582192003727\n",
      "\n",
      "Iteration:1825\n",
      "action: [-0.37396008 -0.65344614]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.065, 0.005, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -124.99803033471107\n",
      "\n",
      "Iteration:1830\n",
      "action: [2.025509   0.17249016]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, 0.001, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -138.9250262528658\n",
      "\n",
      "Iteration:1835\n",
      "action: [0.45612657 1.025658  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.089, -0.002, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -139.8946988284588\n",
      "\n",
      "Iteration:1840\n",
      "action: [0.6971494  0.81562406]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.085, 0.001, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -139.12793722748756\n",
      "\n",
      "Iteration:1845\n",
      "action: [ 0.36326283 -0.873221  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.083, -0.001, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -136.27668198943138\n",
      "\n",
      "Iteration:1850\n",
      "action: [ 0.67758065 -0.06773733]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.088, -0.003, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -138.54227316379547\n",
      "\n",
      "Iteration:1855\n",
      "action: [3.2033179  0.17957161]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.094, -0.007, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -139.37978547811508\n",
      "\n",
      "Iteration:1860\n",
      "action: [-0.08352625 -1.2606972 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.095, -0.007, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -139.8760239481926\n",
      "\n",
      "Iteration:1865\n",
      "action: [-2.2099965 -0.6121313]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.092, -0.017, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -125.1113350391388\n",
      "\n",
      "Iteration:1870\n",
      "action: [0.64012927 1.2673469 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.089, -0.016, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -124.96724826097488\n",
      "\n",
      "Iteration:1875\n",
      "action: [-1.0623752   0.53330445]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.085, -0.005, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -133.5928003191948\n",
      "\n",
      "Iteration:1880\n",
      "action: [-1.2338488 -2.8680072]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.078, 0.005, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -137.86108520627022\n",
      "\n",
      "Iteration:1885\n",
      "action: [-0.5040899  -0.13103363]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.088, -0.004, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -137.72663947939873\n",
      "\n",
      "Iteration:1890\n",
      "action: [-1.5110486 -0.7817299]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.128, -0.028, 1.092, 0.021, -0.029, 1.113]\n",
      "reward: -170.12231919169423\n",
      "\n",
      "Iteration:1895\n",
      "action: [-0.38143235 -1.6769212 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.163, -0.031, 1.059, 0.021, -0.029, 1.113]\n",
      "reward: -240.08095287680626\n",
      "\n",
      "Iteration:1900\n",
      "action: [-1.5935621  1.8242872]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.176, -0.013, 1.025, 0.021, -0.029, 1.113]\n",
      "reward: -301.18395137786865\n",
      "\n",
      "Iteration:1905\n",
      "action: [-0.27365005 -1.4537654 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.176, -0.007, 1.013, 0.021, -0.029, 1.113]\n",
      "reward: -318.29531007632613\n",
      "\n",
      "Iteration:1910\n",
      "action: [0.50308   0.1621447]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.172, -0.017, 1.008, 0.021, -0.029, 1.113]\n",
      "reward: -309.67100086808205\n",
      "\n",
      "Iteration:1915\n",
      "action: [0.20353204 0.5678618 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.169, -0.026, 1.025, 0.021, -0.029, 1.113]\n",
      "reward: -281.21283400058746\n",
      "\n",
      "Iteration:1920\n",
      "action: [1.4335299 2.1056142]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.166, -0.03, 1.041, 0.021, -0.029, 1.113]\n",
      "reward: -260.45312755703924\n",
      "\n",
      "Iteration:1925\n",
      "action: [-0.10985996 -0.69512904]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.153, -0.029, 1.075, 0.021, -0.029, 1.113]\n",
      "reward: -212.1442987114191\n",
      "\n",
      "Iteration:1930\n",
      "action: [-0.10687777 -0.17121758]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.153, -0.032, 1.074, 0.021, -0.029, 1.113]\n",
      "reward: -216.0191836297512\n",
      "\n",
      "Iteration:1935\n",
      "action: [-0.46766117  1.0659803 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.157, -0.039, 1.059, 0.021, -0.029, 1.113]\n",
      "reward: -241.44122027754784\n",
      "\n",
      "Iteration:1940\n",
      "action: [0.38005814 1.0458335 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.155, -0.035, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -228.37804653644562\n",
      "\n",
      "Iteration:1945\n",
      "action: [-0.31023777  1.1877613 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.157, -0.038, 1.058, 0.021, -0.029, 1.113]\n",
      "reward: -243.37327876091004\n",
      "\n",
      "Iteration:1950\n",
      "action: [-3.3240862  0.0347407]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.161, -0.037, 1.05, 0.021, -0.029, 1.113]\n",
      "reward: -253.17034148573876\n",
      "\n",
      "Iteration:1955\n",
      "action: [1.5740095 1.5166837]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.17, -0.023, 1.045, 0.021, -0.029, 1.113]\n",
      "reward: -265.11092606186867\n",
      "\n",
      "Iteration:1960\n",
      "action: [-1.1702951   0.27618167]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.165, -0.029, 1.053, 0.021, -0.029, 1.113]\n",
      "reward: -246.41790823042393\n",
      "\n",
      "Iteration:1965\n",
      "action: [0.2968843 1.0874212]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.159, -0.023, 1.072, 0.021, -0.029, 1.113]\n",
      "reward: -227.54930081963536\n",
      "\n",
      "Iteration:1970\n",
      "action: [ 1.7775408 -2.3580978]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.15, -0.026, 1.081, 0.021, -0.029, 1.113]\n",
      "reward: -206.18306595087049\n",
      "\n",
      "Iteration:1975\n",
      "action: [-0.33516377  0.04641481]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.151, -0.031, 1.077, 0.021, -0.029, 1.113]\n",
      "reward: -210.6309162914753\n",
      "\n",
      "Iteration:1980\n",
      "action: [-0.76494807  1.6549746 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.137, -0.028, 1.093, 0.021, -0.029, 1.113]\n",
      "reward: -179.62826538085935\n",
      "\n",
      "Iteration:1985\n",
      "action: [ 0.30641332 -0.01173327]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.132, -0.027, 1.095, 0.021, -0.029, 1.113]\n",
      "reward: -172.45694372057912\n",
      "\n",
      "Iteration:1990\n",
      "action: [-0.09635944 -0.49828234]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.133, -0.027, 1.095, 0.021, -0.029, 1.113]\n",
      "reward: -174.27101108431813\n",
      "\n",
      "Iteration:1995\n",
      "action: [-0.2492544 -1.0536536]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.132, -0.03, 1.094, 0.021, -0.029, 1.113]\n",
      "reward: -173.76104393005372\n",
      "\n",
      "Iteration:2000\n",
      "action: [0.18507113 1.9930176 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.129, -0.027, 1.097, 0.021, -0.029, 1.113]\n",
      "reward: -168.16888499259946\n",
      "\n",
      "Iteration:2005\n",
      "action: [1.5556599 0.9195205]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.127, -0.03, 1.096, 0.021, -0.029, 1.113]\n",
      "reward: -166.2486458659172\n",
      "\n",
      "Iteration:2010\n",
      "action: [-1.0747372 -0.7443217]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.131, -0.037, 1.088, 0.021, -0.029, 1.113]\n",
      "reward: -185.6362710237503\n",
      "\n",
      "Iteration:2015\n",
      "action: [-1.5058929  1.3098849]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.136, -0.038, 1.083, 0.021, -0.029, 1.113]\n",
      "reward: -195.67786925435067\n",
      "\n",
      "Iteration:2020\n",
      "action: [1.780287   0.17691538]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.143, -0.033, 1.079, 0.021, -0.029, 1.113]\n",
      "reward: -202.0857820749283\n",
      "\n",
      "Iteration:2025\n",
      "action: [ 0.5386414 -0.6051085]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.142, -0.025, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -194.3086453825235\n",
      "\n",
      "Iteration:2030\n",
      "action: [-1.494111  -1.3556551]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.154, -0.026, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -223.70988066494462\n",
      "\n",
      "Iteration:2035\n",
      "action: [-3.260031  -1.5229478]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.162, -0.019, 1.057, 0.021, -0.029, 1.113]\n",
      "reward: -248.55167168378827\n",
      "\n",
      "Iteration:2040\n",
      "action: [0.9040683  0.11811668]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.161, -0.027, 1.05, 0.021, -0.029, 1.113]\n",
      "reward: -247.00541730225083\n",
      "\n",
      "Iteration:2045\n",
      "action: [0.189746  0.5021523]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.139, -0.06, 1.043, 0.021, -0.029, 1.113]\n",
      "reward: -261.7490517437458\n",
      "\n",
      "Iteration:2050\n",
      "action: [ 0.43904215 -0.55189687]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.124, -0.072, 1.035, 0.021, -0.029, 1.113]\n",
      "reward: -265.8614332318306\n",
      "\n",
      "Iteration:2055\n",
      "action: [-0.5894347   0.44604897]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.115, -0.073, 1.008, 0.021, -0.029, 1.113]\n",
      "reward: -285.61625280380247\n",
      "\n",
      "Iteration:2060\n",
      "action: [ 1.2368742  -0.26252303]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.122, -0.073, 1.027, 0.021, -0.029, 1.113]\n",
      "reward: -272.8065664410591\n",
      "\n",
      "Iteration:2065\n",
      "action: [ 1.2379708  -0.84566534]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.119, -0.074, 1.039, 0.021, -0.029, 1.113]\n",
      "reward: -258.54451605081556\n",
      "\n",
      "Iteration:2070\n",
      "action: [-1.1779215   0.30135116]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.119, -0.073, 1.043, 0.021, -0.029, 1.113]\n",
      "reward: -254.0111506819725\n",
      "\n",
      "Iteration:2075\n",
      "action: [-0.92698187  1.2814636 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.121, -0.073, 1.043, 0.021, -0.029, 1.113]\n",
      "reward: -256.33616396188734\n",
      "\n",
      "Iteration:2080\n",
      "action: [ 0.76497567 -1.3455007 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.124, -0.072, 1.035, 0.021, -0.029, 1.113]\n",
      "reward: -266.95284643173215\n",
      "\n",
      "Iteration:2085\n",
      "action: [2.0633574 1.7306323]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.11, -0.079, 1.037, 0.021, -0.029, 1.113]\n",
      "reward: -257.7598879575729\n",
      "\n",
      "Iteration:2090\n",
      "action: [0.07297166 0.26991156]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.102, -0.081, 1.042, 0.021, -0.029, 1.113]\n",
      "reward: -246.77642085552216\n",
      "\n",
      "Iteration:2095\n",
      "action: [ 0.44840592 -0.18429361]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.102, -0.066, 1.079, 0.021, -0.029, 1.113]\n",
      "reward: -194.5297236084938\n",
      "\n",
      "Iteration:2100\n",
      "action: [ 0.9788582  -0.98545194]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.089, -0.071, 1.077, 0.021, -0.029, 1.113]\n",
      "reward: -188.8745839238167\n",
      "\n",
      "Iteration:2105\n",
      "action: [0.05415099 1.8225957 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.078, -0.068, 1.083, 0.021, -0.029, 1.113]\n",
      "reward: -168.14980306625367\n",
      "\n",
      "Iteration:2110\n",
      "action: [0.7116496 1.5456829]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.078, -0.059, 1.094, 0.021, -0.029, 1.113]\n",
      "reward: -148.62386533021927\n",
      "\n",
      "Iteration:2115\n",
      "action: [-0.34712565  0.21918236]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.077, -0.059, 1.094, 0.021, -0.029, 1.113]\n",
      "reward: -147.3236570596695\n",
      "\n",
      "Iteration:2120\n",
      "action: [-0.4636357   0.27387652]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.085, -0.052, 1.098, 0.021, -0.029, 1.113]\n",
      "reward: -143.99423846006394\n",
      "\n",
      "Iteration:2125\n",
      "action: [-0.874943    0.26195565]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.094, -0.033, 1.108, 0.021, -0.029, 1.113]\n",
      "reward: -124.92511072158813\n",
      "\n",
      "Iteration:2130\n",
      "action: [ 2.5827243  -0.82415575]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.104, -0.024, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -134.22030705213544\n",
      "\n",
      "Iteration:2135\n",
      "action: [-0.307334    0.02907676]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.1, -0.019, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -132.33310478925702\n",
      "\n",
      "Iteration:2140\n",
      "action: [-0.44922128 -0.02873122]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.088, 0.001, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -142.73746412992477\n",
      "\n",
      "Iteration:2145\n",
      "action: [0.34070495 0.8074457 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.094, -0.016, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -128.88614934682846\n",
      "\n",
      "Iteration:2150\n",
      "action: [-0.32705942 -0.15173909]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.091, -0.014, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -127.97697734832764\n",
      "\n",
      "Iteration:2155\n",
      "action: [-0.9392792  -0.02024331]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.08, -0.019, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -113.24085396528244\n",
      "\n",
      "Iteration:2160\n",
      "action: [1.566694   0.46165588]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.079, -0.009, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -123.5614727139473\n",
      "\n",
      "Iteration:2165\n",
      "action: [-2.1723711 -1.5251595]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.083, 0.002, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -139.3961916565895\n",
      "\n",
      "Iteration:2170\n",
      "action: [ 0.14490868 -0.507838  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.084, 0.006, 1.117, 0.021, -0.029, 1.113]\n",
      "reward: -143.59586042165756\n",
      "\n",
      "Iteration:2175\n",
      "action: [-0.81195474  0.21811065]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.085, -0.002, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -135.73840361833572\n",
      "\n",
      "Iteration:2180\n",
      "action: [-0.953718    0.37224016]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.091, -0.013, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -129.677304148674\n",
      "\n",
      "Iteration:2185\n",
      "action: [ 0.31834418 -0.85347754]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.096, -0.02, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -127.29894864559171\n",
      "\n",
      "Iteration:2190\n",
      "action: [-0.60348517 -1.3131301 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.097, -0.02, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -128.75625985860822\n",
      "\n",
      "Iteration:2195\n",
      "action: [-1.627418   0.5859572]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.104, -0.021, 1.11, 0.021, -0.029, 1.113]\n",
      "reward: -135.44727611541745\n",
      "\n",
      "Iteration:2200\n",
      "action: [ 0.6449661 -1.564838 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.106, -0.012, 1.112, 0.021, -0.029, 1.113]\n",
      "reward: -145.07452148199079\n",
      "\n",
      "Iteration:2205\n",
      "action: [-0.74650764 -1.2157503 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.102, -0.007, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -146.4225138425827\n",
      "\n",
      "Iteration:2210\n",
      "action: [0.8078552 1.362452 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.102, -0.006, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -147.6223702430725\n",
      "\n",
      "Iteration:2215\n",
      "action: [0.53702796 0.7812464 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.099, 0.001, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -151.92277830839157\n",
      "\n",
      "Iteration:2220\n",
      "action: [1.0055227  0.83736247]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.094, 0.014, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -159.9887619614601\n",
      "\n",
      "Iteration:2225\n",
      "action: [0.80628717 1.4794661 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.085, 0.049, 1.103, 0.021, -0.029, 1.113]\n",
      "reward: -194.62326291203496\n",
      "\n",
      "Iteration:2230\n",
      "action: [-0.73733944  0.3456351 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.102, 0.07, 1.082, 0.021, -0.029, 1.113]\n",
      "reward: -253.04636108875272\n",
      "\n",
      "Iteration:2235\n",
      "action: [0.94933337 0.35812658]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.107, 0.074, 1.074, 0.021, -0.029, 1.113]\n",
      "reward: -269.7461594939232\n",
      "\n",
      "Iteration:2240\n",
      "action: [ 0.7819916 -0.2763904]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.097, 0.073, 1.08, 0.021, -0.029, 1.113]\n",
      "reward: -253.28650075197217\n",
      "\n",
      "Iteration:2245\n",
      "action: [1.1864917 2.0563872]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.087, 0.076, 1.078, 0.021, -0.029, 1.113]\n",
      "reward: -248.0308686494827\n",
      "\n",
      "Iteration:2250\n",
      "action: [-0.5564312  0.5096541]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.099, 0.074, 1.078, 0.021, -0.029, 1.113]\n",
      "reward: -258.7314223051071\n",
      "\n",
      "Iteration:2255\n",
      "action: [-1.2677860e+00  1.9527716e-04]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.097, 0.07, 1.084, 0.021, -0.029, 1.113]\n",
      "reward: -245.53040909767148\n",
      "\n",
      "Iteration:2260\n",
      "action: [0.6834198 1.1154633]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.072, 0.065, 1.09, 0.021, -0.029, 1.113]\n",
      "reward: -210.1440479159355\n",
      "\n",
      "Iteration:2265\n",
      "action: [-1.5263858  -0.20951661]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.069, 0.073, 1.08, 0.021, -0.029, 1.113]\n",
      "reward: -225.18975782394406\n",
      "\n",
      "Iteration:2270\n",
      "action: [ 0.53136253 -0.17842788]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.061, 0.081, 1.063, 0.021, -0.029, 1.113]\n",
      "reward: -242.86052334308621\n",
      "\n",
      "Iteration:2275\n",
      "action: [-1.7103281 -0.9130582]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.081, 0.084, 1.064, 0.021, -0.029, 1.113]\n",
      "reward: -263.926011800766\n",
      "\n",
      "Iteration:2280\n",
      "action: [-0.37945455  0.74068373]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.092, 0.084, 1.062, 0.021, -0.029, 1.113]\n",
      "reward: -276.06335777044296\n",
      "\n",
      "Iteration:2285\n",
      "action: [0.4568639 1.4724218]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.085, 0.086, 1.058, 0.021, -0.029, 1.113]\n",
      "reward: -276.57486993074417\n",
      "\n",
      "Iteration:2290\n",
      "action: [1.889133   0.53917086]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.097, 0.089, 1.04, 0.021, -0.029, 1.113]\n",
      "reward: -308.73556184768677\n",
      "\n",
      "Iteration:2295\n",
      "action: [0.19324733 0.07668441]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.095, 0.09, 1.024, 0.021, -0.029, 1.113]\n",
      "reward: -324.0754355788231\n",
      "\n",
      "Iteration:2300\n",
      "action: [0.27976382 0.39838064]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.078, 0.091, 1.035, 0.021, -0.029, 1.113]\n",
      "reward: -296.42221081256866\n",
      "\n",
      "Iteration:2305\n",
      "action: [-1.0985681 -0.1542733]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.071, 0.09, 1.032, 0.021, -0.029, 1.113]\n",
      "reward: -292.4827312231064\n",
      "\n",
      "Iteration:2310\n",
      "action: [0.42293766 0.52549344]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.047, 0.084, 1.026, 0.021, -0.029, 1.113]\n",
      "reward: -268.3720787167549\n",
      "\n",
      "Iteration:2315\n",
      "action: [-1.163218   1.2418351]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.048, 0.078, 0.997, 0.021, -0.029, 1.113]\n",
      "reward: -291.36410850286484\n",
      "\n",
      "Iteration:2320\n",
      "action: [-1.5265855 -1.2845639]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.015, 0.055, 1.004, 0.021, -0.029, 1.113]\n",
      "reward: -229.86575990170238\n",
      "\n",
      "Iteration:2325\n",
      "action: [-1.5012848  1.9646945]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.02, 1.022, 0.021, -0.029, 1.113]\n",
      "reward: -156.22468508034942\n",
      "\n",
      "Iteration:2330\n",
      "action: [0.2934785 1.2357752]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.003, 0.018, 1.013, 0.021, -0.029, 1.113]\n",
      "reward: -164.60028692334888\n",
      "\n",
      "Iteration:2335\n",
      "action: [0.49666202 1.4670333 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.001, 0.028, 1.006, 0.021, -0.029, 1.113]\n",
      "reward: -183.5417852848768\n",
      "\n",
      "Iteration:2340\n",
      "action: [-1.1076695  -0.26751408]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.025, 0.05, 0.977, 0.021, -0.029, 1.113]\n",
      "reward: -261.0380065739155\n",
      "\n",
      "Iteration:2345\n",
      "action: [-1.8077554   0.12270595]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.026, 0.057, 0.983, 0.021, -0.029, 1.113]\n",
      "reward: -263.0791825354099\n",
      "\n",
      "Iteration:2350\n",
      "action: [-0.8272567 -1.8109614]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.021, 0.049, 0.981, 0.021, -0.029, 1.113]\n",
      "reward: -252.57640454173085\n",
      "\n",
      "Iteration:2355\n",
      "action: [0.8178688  0.32287955]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.004, 0.016, 0.984, 0.021, -0.029, 1.113]\n",
      "reward: -199.28763556480405\n",
      "\n",
      "Iteration:2360\n",
      "action: [ 0.32919908 -0.2040986 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, 0.005, 0.978, 0.021, -0.029, 1.113]\n",
      "reward: -196.04477315396068\n",
      "\n",
      "Iteration:2365\n",
      "action: [-0.8762439  0.450324 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.005, 0.006, 0.98, 0.021, -0.029, 1.113]\n",
      "reward: -193.42397554963824\n",
      "\n",
      "Iteration:2370\n",
      "action: [ 1.0924608 -1.9366672]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.001, 0.011, 0.988, 0.021, -0.029, 1.113]\n",
      "reward: -186.94947595894334\n",
      "\n",
      "Iteration:2375\n",
      "action: [ 0.75315094 -0.382986  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.0, 0.004, 0.989, 0.021, -0.029, 1.113]\n",
      "reward: -176.91916396096346\n",
      "\n",
      "Iteration:2380\n",
      "action: [-0.0042125  1.176829 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.001, 0.005, 0.986, 0.021, -0.029, 1.113]\n",
      "reward: -183.82694089040157\n",
      "\n",
      "Iteration:2385\n",
      "action: [ 0.5875439 -1.5983298]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.0, 0.008, 0.989, 0.021, -0.029, 1.113]\n",
      "reward: -181.7328243628144\n",
      "\n",
      "Iteration:2390\n",
      "action: [0.02065817 0.14510727]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.006, 1.0, 0.021, -0.029, 1.113]\n",
      "reward: -164.25083932653067\n",
      "\n",
      "Iteration:2395\n",
      "action: [-0.8089091 -0.958066 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.006, 0.999, 0.021, -0.029, 1.113]\n",
      "reward: -166.86090995743868\n",
      "\n",
      "Iteration:2400\n",
      "action: [-0.04084731 -0.51451087]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.007, 0.01, 1.015, 0.021, -0.029, 1.113]\n",
      "reward: -151.61961159855124\n",
      "\n",
      "Iteration:2405\n",
      "action: [-0.43713203  0.13595693]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.019, 1.023, 0.021, -0.029, 1.113]\n",
      "reward: -154.76359362900254\n",
      "\n",
      "Iteration:2410\n",
      "action: [-0.75281847 -0.7519758 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.002, 0.027, 1.025, 0.021, -0.029, 1.113]\n",
      "reward: -163.26428489945826\n",
      "\n",
      "Iteration:2415\n",
      "action: [ 0.820339   -0.26979294]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.019, 1.039, 0.021, -0.029, 1.113]\n",
      "reward: -139.37223312631247\n",
      "\n",
      "Iteration:2420\n",
      "action: [-1.0031415   0.74883616]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, 0.014, 1.051, 0.021, -0.029, 1.113]\n",
      "reward: -131.32093968987462\n",
      "\n",
      "Iteration:2425\n",
      "action: [ 0.03102596 -0.40448228]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, 0.014, 1.034, 0.021, -0.029, 1.113]\n",
      "reward: -148.97179260104892\n",
      "\n",
      "Iteration:2430\n",
      "action: [ 0.01311865 -0.57325375]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.013, 0.043, 1.039, 0.021, -0.029, 1.113]\n",
      "reward: -180.30664540082213\n",
      "\n",
      "Iteration:2435\n",
      "action: [-0.00517866  0.06449605]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.03, 0.051, 1.064, 0.021, -0.029, 1.113]\n",
      "reward: -179.1662697792053\n",
      "\n",
      "Iteration:2440\n",
      "action: [-0.36943746 -2.9221847 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.027, 0.039, 1.057, 0.021, -0.029, 1.113]\n",
      "reward: -172.1636665165424\n",
      "\n",
      "Iteration:2445\n",
      "action: [ 1.4495472  -0.18379427]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.038, 0.026, 1.081, 0.021, -0.029, 1.113]\n",
      "reward: -146.71754959225652\n",
      "\n",
      "Iteration:2450\n",
      "action: [-0.81394386 -1.3270918 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.05, 0.037, 1.081, 0.021, -0.029, 1.113]\n",
      "reward: -168.26829063892362\n",
      "\n",
      "Iteration:2455\n",
      "action: [-0.7205303 -0.863029 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.038, 0.02, 1.077, 0.021, -0.029, 1.113]\n",
      "reward: -143.44802248477933\n",
      "\n",
      "Iteration:2460\n",
      "action: [ 0.514604   -0.17517415]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.033, 0.013, 1.078, 0.021, -0.029, 1.113]\n",
      "reward: -131.54111459106204\n",
      "\n",
      "Iteration:2465\n",
      "action: [ 0.22343934 -0.8000991 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.032, 0.016, 1.071, 0.021, -0.029, 1.113]\n",
      "reward: -140.024283260107\n",
      "\n",
      "Iteration:2470\n",
      "action: [-0.28465134 -0.67033184]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.032, 0.03, 1.065, 0.021, -0.029, 1.113]\n",
      "reward: -160.31586314737794\n",
      "\n",
      "Iteration:2475\n",
      "action: [-0.2890622 -0.8444276]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.029, 0.02, 1.07, 0.021, -0.029, 1.113]\n",
      "reward: -141.49393077194688\n",
      "\n",
      "Iteration:2480\n",
      "action: [ 0.3503391 -0.0630386]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.043, 0.015, 1.089, 0.021, -0.029, 1.113]\n",
      "reward: -132.2361922934651\n",
      "\n",
      "Iteration:2485\n",
      "action: [-1.2944587  1.6206876]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.04, 0.022, 1.081, 0.021, -0.029, 1.113]\n",
      "reward: -145.214480638504\n",
      "\n",
      "Iteration:2490\n",
      "action: [ 1.05067   -0.8611975]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.027, 0.012, 1.084, 0.021, -0.029, 1.113]\n",
      "reward: -117.40850965678689\n",
      "\n",
      "Iteration:2495\n",
      "action: [1.041435  0.5499985]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.031, 0.014, 1.093, 0.021, -0.029, 1.113]\n",
      "reward: -115.61145941913125\n",
      "\n",
      "Iteration:2500\n",
      "action: [0.12204482 1.2213756 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.032, 0.006, 1.096, 0.021, -0.029, 1.113]\n",
      "reward: -105.27164238691327\n",
      "\n",
      "Iteration:2505\n",
      "action: [ 0.30652693 -0.01433529]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.037, 0.011, 1.098, 0.021, -0.029, 1.113]\n",
      "reward: -112.96226578950879\n",
      "\n",
      "Iteration:2510\n",
      "action: [0.59504855 0.11556111]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.038, 0.02, 1.094, 0.021, -0.029, 1.113]\n",
      "reward: -126.84213119745252\n",
      "\n",
      "Iteration:2515\n",
      "action: [1.109432  1.0066811]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.028, 0.019, 1.083, 0.021, -0.029, 1.113]\n",
      "reward: -126.27885985374448\n",
      "\n",
      "Iteration:2520\n",
      "action: [-0.8681828 -0.8438963]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.006, 0.008, 1.073, 0.021, -0.029, 1.113]\n",
      "reward: -103.57192001491784\n",
      "\n",
      "Iteration:2525\n",
      "action: [ 2.2689013 -1.7030177]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.003, 0.006, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -98.55761501193045\n",
      "\n",
      "Iteration:2530\n",
      "action: [-0.8541421  -0.14624193]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.007, 0.006, 1.071, 0.021, -0.029, 1.113]\n",
      "reward: -91.27779240906237\n",
      "\n",
      "Iteration:2535\n",
      "action: [ 0.9196253 -1.289949 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.003, 0.013, 1.087, 0.021, -0.029, 1.113]\n",
      "reward: -92.46964502334593\n",
      "\n",
      "Iteration:2540\n",
      "action: [0.52474576 0.05952157]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.014, 0.011, 1.088, 0.021, -0.029, 1.113]\n",
      "reward: -100.35912442207334\n",
      "\n",
      "Iteration:2545\n",
      "action: [ 0.22032191 -1.8618133 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.013, 0.007, 1.086, 0.021, -0.029, 1.113]\n",
      "reward: -96.99286177009341\n",
      "\n",
      "Iteration:2550\n",
      "action: [-0.8350735   0.09922771]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.042, 0.009, 1.111, 0.021, -0.029, 1.113]\n",
      "reward: -103.61639904975888\n",
      "\n",
      "Iteration:2555\n",
      "action: [-1.7816507  -0.52234566]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.058, 0.004, 1.114, 0.021, -0.029, 1.113]\n",
      "reward: -112.84371753036976\n",
      "\n",
      "Iteration:2560\n",
      "action: [-1.2355504  0.8618672]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.117, 0.001, 1.108, 0.021, -0.029, 1.113]\n",
      "reward: -173.0564290769398\n",
      "\n",
      "Iteration:2565\n",
      "action: [ 0.31989744 -0.1498135 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.131, 0.001, 1.106, 0.021, -0.029, 1.113]\n",
      "reward: -189.46026387810704\n",
      "\n",
      "Iteration:2570\n",
      "action: [ 0.6034684 -0.3394655]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.144, -0.004, 1.098, 0.021, -0.029, 1.113]\n",
      "reward: -204.8212581425905\n",
      "\n",
      "Iteration:2575\n",
      "action: [-1.1722916  1.6188346]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.152, -0.02, 1.089, 0.021, -0.029, 1.113]\n",
      "reward: -205.54739731550214\n",
      "\n",
      "Iteration:2580\n",
      "action: [-0.9129037  0.6717537]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.146, -0.011, 1.098, 0.021, -0.029, 1.113]\n",
      "reward: -199.8674788624048\n",
      "\n",
      "Iteration:2585\n",
      "action: [0.04526468 1.2180121 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.135, -0.004, 1.104, 0.021, -0.029, 1.113]\n",
      "reward: -189.730190962553\n",
      "\n",
      "Iteration:2590\n",
      "action: [1.161808  0.5987048]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.138, -0.004, 1.102, 0.021, -0.029, 1.113]\n",
      "reward: -194.48788586258885\n",
      "\n",
      "Iteration:2595\n",
      "action: [-0.02864237 -0.26811492]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.143, -0.006, 1.099, 0.021, -0.029, 1.113]\n",
      "reward: -201.65038976073262\n",
      "\n",
      "Iteration:2600\n",
      "action: [-1.2492964   0.27832133]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.146, -0.003, 1.097, 0.021, -0.029, 1.113]\n",
      "reward: -208.11563945561645\n",
      "\n",
      "Iteration:2605\n",
      "action: [0.9073478  0.26604265]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.144, 0.0, 1.1, 0.021, -0.029, 1.113]\n",
      "reward: -207.0018782392144\n",
      "\n",
      "Iteration:2610\n",
      "action: [0.08937663 1.6047207 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.135, -0.0, 1.105, 0.021, -0.029, 1.113]\n",
      "reward: -193.48233827576038\n",
      "\n",
      "Iteration:2615\n",
      "action: [-0.29297554  1.4958743 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.126, -0.0, 1.109, 0.021, -0.029, 1.113]\n",
      "reward: -179.58396422863004\n",
      "\n",
      "Iteration:2620\n",
      "action: [-2.257223  1.258398]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.128, -0.005, 1.108, 0.021, -0.029, 1.113]\n",
      "reward: -177.4776292145252\n",
      "\n",
      "Iteration:2625\n",
      "action: [-1.3353069  0.6347779]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.104, 0.002, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -158.48854285478592\n",
      "\n",
      "Iteration:2630\n",
      "action: [ 0.8794188 -0.7993005]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.054, 0.014, 1.113, 0.021, -0.029, 1.113]\n",
      "reward: -118.32909128814933\n",
      "\n",
      "Iteration:2635\n",
      "action: [ 0.26522794 -1.5693685 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.048, 0.015, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -114.44961380958557\n",
      "\n",
      "Iteration:2640\n",
      "action: [ 0.23996472 -0.32069513]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.048, 0.016, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -115.64733934402466\n",
      "\n",
      "Iteration:2645\n",
      "action: [1.0810182 1.6578734]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.061, 0.022, 1.116, 0.021, -0.029, 1.113]\n",
      "reward: -135.2081513851881\n",
      "\n",
      "Iteration:2650\n",
      "action: [-1.6281016  -0.20067461]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.057, 0.025, 1.115, 0.021, -0.029, 1.113]\n",
      "reward: -133.13017328083515\n",
      "\n",
      "Iteration:2655\n",
      "action: [0.81779075 0.83009624]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.067, 0.016, 1.118, 0.021, -0.029, 1.113]\n",
      "reward: -138.6073875129223\n",
      "\n",
      "Iteration:2660\n",
      "action: [-1.9882988  1.2241484]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.057, 0.039, 1.107, 0.021, -0.029, 1.113]\n",
      "reward: -152.263292402029\n",
      "\n",
      "Iteration:2665\n",
      "action: [0.06129171 0.21979448]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.034, 0.041, 1.098, 0.021, -0.029, 1.113]\n",
      "reward: -139.51357948780057\n",
      "\n",
      "Iteration:2670\n",
      "action: [0.44241327 1.3216884 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.031, 0.053, 1.085, 0.021, -0.029, 1.113]\n",
      "reward: -162.52226877212522\n",
      "\n",
      "Iteration:2675\n",
      "action: [0.54758626 0.6740647 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.028, 0.073, 1.028, 0.021, -0.029, 1.113]\n",
      "reward: -235.52727821469304\n",
      "\n",
      "Iteration:2680\n",
      "action: [ 0.80325294 -1.5449761 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.034, 0.063, 0.986, 0.021, -0.029, 1.113]\n",
      "reward: -274.31592839956284\n",
      "\n",
      "Iteration:2685\n",
      "action: [-0.4496951  1.3848084]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.031, 0.072, 1.004, 0.021, -0.029, 1.113]\n",
      "reward: -261.43288362026215\n",
      "\n",
      "Iteration:2690\n",
      "action: [ 1.0076709  -0.43875253]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.02, 0.066, 1.008, 0.021, -0.029, 1.113]\n",
      "reward: -240.88933068513867\n",
      "\n",
      "Iteration:2695\n",
      "action: [ 0.30549413 -0.21767345]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.018, 0.068, 1.018, 0.021, -0.029, 1.113]\n",
      "reward: -231.5701761394739\n",
      "\n",
      "Iteration:2700\n",
      "action: [-1.8773234  0.4404772]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.027, 0.074, 1.021, 0.021, -0.029, 1.113]\n",
      "reward: -243.22464253008363\n",
      "\n",
      "Iteration:2705\n",
      "action: [-1.7808836  0.4406166]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.029, 0.065, 0.991, 0.021, -0.029, 1.113]\n",
      "reward: -265.90752351284027\n",
      "\n",
      "Iteration:2710\n",
      "action: [-1.4672179 -0.9595358]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.026, 0.063, 0.991, 0.021, -0.029, 1.113]\n",
      "reward: -261.55539724230766\n",
      "\n",
      "Iteration:2715\n",
      "action: [-0.54899025 -1.1945081 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.013, 0.066, 1.02, 0.021, -0.029, 1.113]\n",
      "reward: -220.81978368759152\n",
      "\n",
      "Iteration:2720\n",
      "action: [ 1.0868211 -0.5744111]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.044, 1.042, 0.021, -0.029, 1.113]\n",
      "reward: -160.1241010352969\n",
      "\n",
      "Iteration:2725\n",
      "action: [-3.180913  -1.5150282]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.012, 0.057, 1.056, 0.021, -0.029, 1.113]\n",
      "reward: -175.81432993710038\n",
      "\n",
      "Iteration:2730\n",
      "action: [-0.79451585  0.7218951 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.015, 0.062, 1.051, 0.021, -0.029, 1.113]\n",
      "reward: -188.21329931914804\n",
      "\n",
      "Iteration:2735\n",
      "action: [-0.73319525 -1.0348374 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.0, 0.047, 1.05, 0.021, -0.029, 1.113]\n",
      "reward: -160.6881546899676\n",
      "\n",
      "Iteration:2740\n",
      "action: [0.9851781  0.04888942]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.026, 1.061, 0.021, -0.029, 1.113]\n",
      "reward: -123.33352240920065\n",
      "\n",
      "Iteration:2745\n",
      "action: [ 0.004622  -0.7974359]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.003, 0.041, 1.064, 0.021, -0.029, 1.113]\n",
      "reward: -142.72277738153932\n",
      "\n",
      "Iteration:2750\n",
      "action: [-0.44764322  0.33220813]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.003, 0.045, 1.06, 0.021, -0.029, 1.113]\n",
      "reward: -150.611873075366\n",
      "\n",
      "Iteration:2755\n",
      "action: [ 1.7459217 -0.6690141]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.031, 1.058, 0.021, -0.029, 1.113]\n",
      "reward: -130.80335530638692\n",
      "\n",
      "Iteration:2760\n",
      "action: [1.3354388  0.46032286]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.004, 0.036, 1.054, 0.021, -0.029, 1.113]\n",
      "reward: -141.7834472954273\n",
      "\n",
      "Iteration:2765\n",
      "action: [0.5636411  0.24562591]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.002, 0.046, 1.047, 0.021, -0.029, 1.113]\n",
      "reward: -159.89358752220866\n",
      "\n",
      "Iteration:2770\n",
      "action: [-0.6150754  1.5909753]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.004, 0.054, 1.05, 0.021, -0.029, 1.113]\n",
      "reward: -171.66144150495526\n",
      "\n",
      "Iteration:2775\n",
      "action: [-0.84913844  1.7156807 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.007, 0.04, 1.037, 0.021, -0.029, 1.113]\n",
      "reward: -159.09167361631987\n",
      "\n",
      "Iteration:2780\n",
      "action: [0.21259533 0.77858335]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.007, 0.041, 1.026, 0.021, -0.029, 1.113]\n",
      "reward: -170.68388900905845\n",
      "\n",
      "Iteration:2785\n",
      "action: [-1.5876119  0.9337105]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.032, 1.022, 0.021, -0.029, 1.113]\n",
      "reward: -160.97365255653855\n",
      "\n",
      "Iteration:2790\n",
      "action: [-0.34012842  0.10737946]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.015, 0.011, 1.008, 0.021, -0.029, 1.113]\n",
      "reward: -150.3427990823984\n",
      "\n",
      "Iteration:2795\n",
      "action: [ 2.5036368 -1.070304 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.015, 0.003, 1.006, 0.021, -0.029, 1.113]\n",
      "reward: -145.06985811702904\n",
      "\n",
      "Iteration:2800\n",
      "action: [-0.6654435 -1.0531056]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.013, 0.013, 1.002, 0.021, -0.029, 1.113]\n",
      "reward: -161.2583876699209\n",
      "\n",
      "Iteration:2805\n",
      "action: [-0.6624409  -0.05238406]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.016, 0.013, 1.009, 0.021, -0.029, 1.113]\n",
      "reward: -151.5400980710983\n",
      "\n",
      "Iteration:2810\n",
      "action: [ 0.7672426 -0.3263209]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.017, 0.005, 1.009, 0.021, -0.029, 1.113]\n",
      "reward: -141.75557746738193\n",
      "\n",
      "Iteration:2815\n",
      "action: [-0.21631432 -0.5507535 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.015, 0.011, 1.005, 0.021, -0.029, 1.113]\n",
      "reward: -154.90018512308595\n",
      "\n",
      "Iteration:2820\n",
      "action: [1.5290971  0.43648642]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.007, 0.002, 0.986, 0.021, -0.029, 1.113]\n",
      "reward: -171.79117927979675\n",
      "\n",
      "Iteration:2825\n",
      "action: [0.83936214 0.52760345]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.003, 0.987, 0.021, -0.029, 1.113]\n",
      "reward: -169.62508956156668\n",
      "\n",
      "Iteration:2830\n",
      "action: [-1.8221691   0.04298124]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.002, 0.98, 0.021, -0.029, 1.113]\n",
      "reward: -179.63696020841596\n",
      "\n",
      "Iteration:2835\n",
      "action: [-0.4563022 -1.4251351]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.011, 0.995, 0.021, -0.029, 1.113]\n",
      "reward: -166.6258194595575\n",
      "\n",
      "Iteration:2840\n",
      "action: [ 0.37923658 -0.92059237]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.011, 0.995, 0.021, -0.029, 1.113]\n",
      "reward: -166.4177144989371\n",
      "\n",
      "Iteration:2845\n",
      "action: [-0.8265336 -1.0324897]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.016, 0.008, 1.006, 0.021, -0.029, 1.113]\n",
      "reward: -149.37506924569604\n",
      "\n",
      "Iteration:2850\n",
      "action: [-0.04436093 -0.75266683]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.017, -0.001, 1.01, 0.021, -0.029, 1.113]\n",
      "reward: -135.34509739791972\n",
      "\n",
      "Iteration:2855\n",
      "action: [-0.03571337 -0.43812263]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.018, 0.002, 1.018, 0.021, -0.029, 1.113]\n",
      "reward: -128.80977644771335\n",
      "\n",
      "Iteration:2860\n",
      "action: [-0.4911655   0.43484047]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.017, 0.004, 1.038, 0.021, -0.029, 1.113]\n",
      "reward: -112.21628339216112\n",
      "\n",
      "Iteration:2865\n",
      "action: [-0.9073834   0.63363165]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.017, 1.042, 0.021, -0.029, 1.113]\n",
      "reward: -124.93878725171088\n",
      "\n",
      "Iteration:2870\n",
      "action: [-1.0618109  0.2944355]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.006, 1.062, 0.021, -0.029, 1.113]\n",
      "reward: -96.96201450377701\n",
      "\n",
      "Iteration:2875\n",
      "action: [ 0.43796676 -1.6376011 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.003, 0.003, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -95.26243602111934\n",
      "\n",
      "Iteration:2880\n",
      "action: [-0.64081657 -0.5514433 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, -0.003, 1.067, 0.021, -0.029, 1.113]\n",
      "reward: -88.94235320575534\n",
      "\n",
      "Iteration:2885\n",
      "action: [ 0.42756516 -0.5253425 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.003, 0.006, 1.078, 0.021, -0.029, 1.113]\n",
      "reward: -93.8055038563907\n",
      "\n",
      "Iteration:2890\n",
      "action: [ 0.43012893 -0.32906976]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.0, -0.001, 1.076, 0.021, -0.029, 1.113]\n",
      "reward: -87.31721132015808\n",
      "\n",
      "Iteration:2895\n",
      "action: [-1.8964342  1.1215904]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.002, -0.001, 1.071, 0.021, -0.029, 1.113]\n",
      "reward: -88.2708329586312\n",
      "\n",
      "Iteration:2900\n",
      "action: [-0.0233921  1.1204627]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.005, 0.01, 1.057, 0.021, -0.029, 1.113]\n",
      "reward: -110.88179732859133\n",
      "\n",
      "Iteration:2905\n",
      "action: [-0.27955586 -0.3815073 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.008, 0.0, 1.025, 0.021, -0.029, 1.113]\n",
      "reward: -130.16284996364263\n",
      "\n",
      "Iteration:2910\n",
      "action: [ 0.08053975 -0.77176434]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.005, 1.015, 0.021, -0.029, 1.113]\n",
      "reward: -141.01892086863515\n",
      "\n",
      "Iteration:2915\n",
      "action: [0.2885291  0.70528907]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.003, 0.038, 1.018, 0.021, -0.029, 1.113]\n",
      "reward: -181.03175359964368\n",
      "\n",
      "Iteration:2920\n",
      "action: [-0.47807327  0.57578814]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.018, 0.058, 0.998, 0.021, -0.029, 1.113]\n",
      "reward: -240.72153988480565\n",
      "\n",
      "Iteration:2925\n",
      "action: [0.22615407 0.397687  ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.028, 0.056, 0.982, 0.021, -0.029, 1.113]\n",
      "reward: -264.7158981412649\n",
      "\n",
      "Iteration:2930\n",
      "action: [ 0.16249907 -0.47061327]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.032, 0.058, 0.98, 0.021, -0.029, 1.113]\n",
      "reward: -273.2179102599621\n",
      "\n",
      "Iteration:2935\n",
      "action: [-0.07742252  0.04820713]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.025, 0.066, 0.998, 0.021, -0.029, 1.113]\n",
      "reward: -256.5899652540684\n",
      "\n",
      "Iteration:2940\n",
      "action: [-1.3460308   0.92549264]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.026, 0.068, 1.001, 0.021, -0.029, 1.113]\n",
      "reward: -255.873889952898\n",
      "\n",
      "Iteration:2945\n",
      "action: [-0.8474491  1.078953 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.021, 0.063, 0.998, 0.021, -0.029, 1.113]\n",
      "reward: -249.4080414175987\n",
      "\n",
      "Iteration:2950\n",
      "action: [-1.4121654 -1.4437213]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.014, 0.061, 1.005, 0.021, -0.029, 1.113]\n",
      "reward: -232.69944108277556\n",
      "\n",
      "Iteration:2955\n",
      "action: [ 0.33362228 -0.3996135 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.003, 0.048, 1.008, 0.021, -0.029, 1.113]\n",
      "reward: -205.80320834368464\n",
      "\n",
      "Iteration:2960\n",
      "action: [ 1.4155933  -0.36671627]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.001, 0.044, 1.011, 0.021, -0.029, 1.113]\n",
      "reward: -194.43444191664454\n",
      "\n",
      "Iteration:2965\n",
      "action: [-0.40417653  1.2145901 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.007, 0.035, 1.013, 0.021, -0.029, 1.113]\n",
      "reward: -178.0739252567291\n",
      "\n",
      "Iteration:2970\n",
      "action: [ 0.6279868 -1.1792456]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.015, 0.009, 1.016, 0.021, -0.029, 1.113]\n",
      "reward: -140.4406738579273\n",
      "\n",
      "Iteration:2975\n",
      "action: [-0.27186498  1.6460553 ]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.012, 0.002, 1.036, 0.021, -0.029, 1.113]\n",
      "reward: -116.65097816102205\n",
      "\n",
      "Iteration:2980\n",
      "action: [-0.47003648 -0.49720767]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.004, 1.026, 0.021, -0.029, 1.113]\n",
      "reward: -131.34294823929665\n",
      "\n",
      "Iteration:2985\n",
      "action: [-0.14744246 -0.19112082]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.011, 0.009, 1.031, 0.021, -0.029, 1.113]\n",
      "reward: -129.9125638902187\n",
      "\n",
      "Iteration:2990\n",
      "action: [ 0.637469  -0.7111495]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [0.01, 0.003, 1.048, 0.021, -0.029, 1.113]\n",
      "reward: -107.97123593464492\n",
      "\n",
      "Iteration:2995\n",
      "action: [0.17158064 0.02519594]\n",
      "goal: [0.0212, -0.0288, 1.113]\n",
      "state: [-0.016, 0.013, 1.088, 0.021, -0.029, 1.113]\n",
      "reward: -104.08080212026832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for i in range(3000):\n",
    "    action, _state = ppo_model.predict(obs, deterministic=False)\n",
    "    # print(_state)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    # applied_action = env._action_mapping_torque(action)\n",
    "    \n",
    "    if i%5==0:\n",
    "        print(f\"Iteration:{i}\")\n",
    "        print(f\"action: {action}\")\n",
    "        print(f\"goal: {goal[0]['position'][7]}\")\n",
    "        print(f\"state: {[round(ob,3) for ob in obs]}\")\n",
    "        print(f\"reward: {reward}\\n\")\n",
    "    \n",
    "    if done: \n",
    "        print(\"done\")\n",
    "        break\n",
    "    \n",
    "    # sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Mar 29 2022 22:56:14\n",
      "/Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/configuration.py:72: UserWarning: No agent/task config given.\n",
      "  warnings.warn(\"No agent/task config given.\")\n"
     ]
    }
   ],
   "source": [
    "from safe_control_gym.envs.manipulators.manipulator import BaseManipulator\n",
    "import numpy as np \n",
    "import random \n",
    "from time import sleep\n",
    "from safe_control_gym.envs.benchmark_env import Cost, Task\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "import os \n",
    "home = \"/Users/nicholasprayogo/code/rm-sc\"\n",
    "# cwd = os.path.join(home, \"src/interpretable_ts_clustering/\")\n",
    "os.chdir(home)\n",
    "\n",
    "from functools import partial\n",
    "from safe_control_gym.utils.configuration import ConfigFactory\n",
    "\n",
    "fac = ConfigFactory()\n",
    "config = fac.merge()\n",
    "# env_func = partial(make, config.task, output_dir=config.output_dir, **config.task_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Munch({'tag': 'temp', 'seed': None, 'device': 'cpu', 'output_dir': 'results', 'restore': None})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config.algo = \"lqr\"\n",
    "config.task = \"manipulator\"\n",
    "controller_yaml_path = \"safe_control_gym/controllers/lqr/lqr.yaml\"\n",
    "env_yaml_path = \"safe_control_gym/envs/manipulators/manipulator.yaml\"\n",
    "\n",
    "with open(controller_yaml_path, \"r\") as yamlfile:\n",
    "    config.algo_config = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "with open(env_yaml_path, \"r\") as yamlfile:\n",
    "    config.task_config = yaml.load(yamlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@1=vertcat(cos(theta), sin(theta)), (mac(mac((0.5*(@1-Xr)'),Q,zeros(1x2)),(@1-Xr),0)+(((0.5*(U-Ur))*R)*(U-Ur)))argv[0]=\n",
      "Version = 4.1 Metal - 76.3\n",
      "Vendor = Apple\n",
      "Renderer = Apple M1 Pro\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in Function::Function for 'fc' [MXFunction] at .../casadi/core/function.cpp:235:\n.../casadi/core/function_internal.cpp:145: Error calling MXFunction::init for 'fc':\n.../casadi/core/x_function.hpp:287: For fc: Xfunction input arguments must be purely symbolic.\nArgument 0(x) is not symbolic.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msafe_control_gym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistration\u001b[39;00m \u001b[39mimport\u001b[39;00m make\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=1'>2</a>\u001b[0m env_func \u001b[39m=\u001b[39m partial(make, config\u001b[39m.\u001b[39mtask, output_dir\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39moutput_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig\u001b[39m.\u001b[39mtask_config)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=2'>3</a>\u001b[0m control_agent \u001b[39m=\u001b[39m make(config\u001b[39m.\u001b[39;49malgo,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=3'>4</a>\u001b[0m                          env_func,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=4'>5</a>\u001b[0m                          training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=5'>6</a>\u001b[0m                          checkpoint_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(config\u001b[39m.\u001b[39;49moutput_dir, \u001b[39m\"\u001b[39;49m\u001b[39mmodel_latest_manippulator.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=6'>7</a>\u001b[0m                          output_dir\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49moutput_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=7'>8</a>\u001b[0m                          device\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=8'>9</a>\u001b[0m                          seed\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000032?line=9'>10</a>\u001b[0m                          \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig\u001b[39m.\u001b[39;49malgo_config)\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/utils/registration.py:149\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=144'>145</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=145'>146</a>\u001b[0m     \u001b[39m\"\"\"Creates an instance of the callable from global registry.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=146'>147</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=147'>148</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=148'>149</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39;49mmake(\u001b[39mid\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/utils/registration.py:102\u001b[0m, in \u001b[0;36mRegistry.make\u001b[0;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=97'>98</a>\u001b[0m \u001b[39m\"\"\"Create an instance of the registered callable by id `path`.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=98'>99</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=99'>100</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=100'>101</a>\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspecs[path]\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=101'>102</a>\u001b[0m obj \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39;49mmake(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=102'>103</a>\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/utils/registration.py:79\u001b[0m, in \u001b[0;36mSpec.make\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=76'>77</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=77'>78</a>\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_point)\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=78'>79</a>\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=79'>80</a>\u001b[0m \u001b[39m# Make the instance aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=80'>81</a>\u001b[0m spec \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py:96\u001b[0m, in \u001b[0;36mLQR.__init__\u001b[0;34m(self, env_func, q_lqr, r_lqr, discrete_dynamics, deque_size, eval_batch_size, task, task_info, episode_len_sec, output_dir, verbose, model_step_chk, random_init, ctrl_freq, pyb_freq, save_data, data_dir, plot_traj, plot_dir, save_plot, init_state_randomization_info, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=93'>94</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeque_size \u001b[39m=\u001b[39m deque_size\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=94'>95</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_init \u001b[39m=\u001b[39m random_init\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=95'>96</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env_func(cost\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcost,\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=96'>97</a>\u001b[0m                     randomized_init\u001b[39m=\u001b[39;49mrandom_init,\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=97'>98</a>\u001b[0m                     init_state_randomization_info\u001b[39m=\u001b[39;49minit_state_randomization_info,\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=98'>99</a>\u001b[0m                     randomized_inertial_prop\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=99'>100</a>\u001b[0m                     episode_len_sec\u001b[39m=\u001b[39;49mepisode_len_sec,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=100'>101</a>\u001b[0m                     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=101'>102</a>\u001b[0m                     task_info\u001b[39m=\u001b[39;49mtask_info,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=102'>103</a>\u001b[0m                     ctrl_freq\u001b[39m=\u001b[39;49mctrl_freq,\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=103'>104</a>\u001b[0m                     pyb_freq\u001b[39m=\u001b[39;49mpyb_freq)\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=104'>105</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m RecordEpisodeStatistics(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, deque_size)\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/controllers/lqr/lqr.py?line=106'>107</a>\u001b[0m \u001b[39m# Controller params.\u001b[39;00m\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/utils/registration.py:149\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=144'>145</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=145'>146</a>\u001b[0m     \u001b[39m\"\"\"Creates an instance of the callable from global registry.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=146'>147</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=147'>148</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=148'>149</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39;49mmake(\u001b[39mid\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/utils/registration.py:102\u001b[0m, in \u001b[0;36mRegistry.make\u001b[0;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=97'>98</a>\u001b[0m \u001b[39m\"\"\"Create an instance of the registered callable by id `path`.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=98'>99</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=99'>100</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=100'>101</a>\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspecs[path]\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=101'>102</a>\u001b[0m obj \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39;49mmake(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=102'>103</a>\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/utils/registration.py:79\u001b[0m, in \u001b[0;36mSpec.make\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=76'>77</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=77'>78</a>\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_point)\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=78'>79</a>\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=79'>80</a>\u001b[0m \u001b[39m# Make the instance aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/utils/registration.py?line=80'>81</a>\u001b[0m spec \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py:119\u001b[0m, in \u001b[0;36mBaseManipulator.__init__\u001b[0;34m(self, urdf_path, controlled_variable, control_method, target_space, controlled_joint_indices, observed_link_indices, observed_link_state_keys, goal, goal_type, dimensions, connection, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=116'>117</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGRAVITY_ACC \u001b[39m=\u001b[39m \u001b[39m9.8\u001b[39m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=117'>118</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=118'>119</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_symbolic()\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py:300\u001b[0m, in \u001b[0;36mBaseManipulator._setup_symbolic\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=297'>298</a>\u001b[0m dynamics \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdyn_eqn\u001b[39m\u001b[39m\"\u001b[39m: Theta_dot_dot, \u001b[39m\"\u001b[39m\u001b[39mobs_eqn\u001b[39m\u001b[39m\"\u001b[39m: Y, \u001b[39m\"\u001b[39m\u001b[39mvars\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: X, \u001b[39m\"\u001b[39m\u001b[39mU\u001b[39m\u001b[39m\"\u001b[39m: U}}\n\u001b[1;32m    <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=298'>299</a>\u001b[0m cost \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcost_func\u001b[39m\u001b[39m\"\u001b[39m: cost_func, \u001b[39m\"\u001b[39m\u001b[39mvars\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: X, \u001b[39m\"\u001b[39m\u001b[39mU\u001b[39m\u001b[39m\"\u001b[39m: U, \u001b[39m\"\u001b[39m\u001b[39mXr\u001b[39m\u001b[39m\"\u001b[39m: Xr, \u001b[39m\"\u001b[39m\u001b[39mUr\u001b[39m\u001b[39m\"\u001b[39m: Ur, \u001b[39m\"\u001b[39m\u001b[39mQ\u001b[39m\u001b[39m\"\u001b[39m: Q, \u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m: R}}\n\u001b[0;32m--> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/envs/manipulators/manipulator.py?line=299'>300</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msymbolic \u001b[39m=\u001b[39m SymbolicModel(dynamics\u001b[39m=\u001b[39;49mdynamics, cost\u001b[39m=\u001b[39;49mcost, dt\u001b[39m=\u001b[39;49mdt)\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py:59\u001b[0m, in \u001b[0;36mSymbolicModel.__init__\u001b[0;34m(self, dynamics, cost, dt, integration_algo, funcs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=56'>57</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mUr \u001b[39m=\u001b[39m cost[\u001b[39m\"\u001b[39m\u001b[39mvars\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mUr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=57'>58</a>\u001b[0m \u001b[39m# Setup symbolic model.\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_model()\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=59'>60</a>\u001b[0m \u001b[39m# Setup Jacobian and Hessian of the dynamics and cost functions.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_linearization()\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py:68\u001b[0m, in \u001b[0;36mSymbolicModel.setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=63'>64</a>\u001b[0m \u001b[39m\"\"\"Exposes functions to evaluate the model.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=64'>65</a>\u001b[0m \n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=65'>66</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=66'>67</a>\u001b[0m \u001b[39m# Continuous time dynamics.\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=67'>68</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_func \u001b[39m=\u001b[39m cs\u001b[39m.\u001b[39;49mFunction(\u001b[39m'\u001b[39;49m\u001b[39mfc\u001b[39;49m\u001b[39m'\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_sym, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mu_sym], [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_dot], [\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m], [\u001b[39m'\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=68'>69</a>\u001b[0m \u001b[39m# Discrete time dynamics.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=69'>70</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfd_func \u001b[39m=\u001b[39m cs\u001b[39m.\u001b[39mintegrator(\u001b[39m'\u001b[39m\u001b[39mfd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintegration_algo, {\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_sym,\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=70'>71</a>\u001b[0m                                                            \u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu_sym,\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=71'>72</a>\u001b[0m                                                            \u001b[39m'\u001b[39m\u001b[39mode\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_dot}, {\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt}\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=72'>73</a>\u001b[0m                             )\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py:13558\u001b[0m, in \u001b[0;36mFunction.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13465'>13466</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13466'>13467</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13467'>13468</a>\u001b[0m \n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13468'>13469</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13555'>13556</a>\u001b[0m \n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13556'>13557</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13557'>13558</a>\u001b[0m     this \u001b[39m=\u001b[39m _casadi\u001b[39m.\u001b[39;49mnew_Function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13558'>13559</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13559'>13560</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthis\u001b[39m.\u001b[39mappend(this)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in Function::Function for 'fc' [MXFunction] at .../casadi/core/function.cpp:235:\n.../casadi/core/function_internal.cpp:145: Error calling MXFunction::init for 'fc':\n.../casadi/core/x_function.hpp:287: For fc: Xfunction input arguments must be purely symbolic.\nArgument 0(x) is not symbolic."
     ]
    }
   ],
   "source": [
    "from safe_control_gym.utils.registration import make\n",
    "env_func = partial(make, config.task, output_dir=config.output_dir, **config.task_config)\n",
    "control_agent = make(config.algo,\n",
    "                         env_func,\n",
    "                         training=True,\n",
    "                         checkpoint_path=os.path.join(config.output_dir, \"model_latest_manippulator.pt\"),\n",
    "                         output_dir=config.output_dir,\n",
    "                         device=config.device,\n",
    "                         seed=config.seed,\n",
    "                         **config.algo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function make at 0x7fb1e07111f0>, 'lqr_manipulator', output_dir='results', test='test')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safe_control_gym.utils.registration import register\n",
    "\n",
    "register(id=\"manipulator\",\n",
    "         entry_point=\"safe_control_gym.envs.manipulators.manipulator:BaseManipulator\",\n",
    "         config_entry_point=\"safe_control_gym.envs.manipulators:manipulator.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh: Can't add module parameter `commands': parameter already exists\n",
      "CITATION.cff     \u001b[34mexamples\u001b[m\u001b[m         pyproject.toml   setup.py\n",
      "LICENSE          \u001b[34mexperiments\u001b[m\u001b[m      \u001b[34mresults\u001b[m\u001b[m          \u001b[34mwalkthroughs\u001b[m\u001b[m\n",
      "README.md        \u001b[34mfigures\u001b[m\u001b[m          \u001b[34msafe_control_gym\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!python3 experiments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import casadi as cs\n",
    "\n",
    "g = 9.8\n",
    "dt = 0.005 \n",
    "mc = 0.5\n",
    "ma = 0.5\n",
    "# TODO express in terms of actual length (distance b.w 2 joints)\n",
    "l = 1\n",
    "I = 0 # moment of inertia from gears and motors\n",
    "\n",
    "# must match dimension of X_GOAL and U_GOAL\n",
    "nx = 2\n",
    "nu = 1 \n",
    "\n",
    "# x = cs.MX.sym('x')\n",
    "\n",
    "x_x = cs.MX.sym('x_x')\n",
    "# x_y = cs.MX.sym('x_y')\n",
    "x_z = cs.MX.sym('x_z')\n",
    "theta = cs.MX.sym('theta')\n",
    "U = cs.MX.sym('U') # torque\n",
    "theta_dot_dot = cs.MX.sym(\"tdotdot\")\n",
    "Theta_dot_dot = cs.vertcat(theta_dot_dot, (U-(mc/2+ma) * g * l * cs.sin(theta)) / ((mc/3 + ma)* l**2 + I))\n",
    "# Theta_dot_dot = cs.MX.sym(\"ttt\")\n",
    "# not sure\n",
    "# X = cs.vertcat(theta)\n",
    "X = cs.vertcat(theta)\n",
    "Y = cs.vertcat(theta)\n",
    "\n",
    "Q = cs.MX.sym('Q', nx, nx)\n",
    "R = cs.MX.sym('R', nu, nu)\n",
    "\n",
    "Xr = cs.MX.sym('Xr', nx, 1)\n",
    "Ur = cs.MX.sym('Ur', nu, 1)\n",
    "\n",
    "# convert angular to cartesian position \n",
    "x_x = l * cs.cos(theta)\n",
    "x_z = l * cs.sin(theta) \n",
    "\n",
    "X_array = cs.vertcat(x_x, x_z)\n",
    "\n",
    "cost_func = 0.5 * (X_array - Xr).T @ Q @ (X_array - Xr) + 0.5 * (U - Ur).T @ R @ (U - Ur)\n",
    "\n",
    "dynamics = {\"dyn_eqn\": Theta_dot_dot, \"obs_eqn\": Y, \"vars\": {\"X\": X, \"U\": U}}\n",
    "cost = {\"cost_func\": cost_func, \"vars\": {\"X\": X, \"U\": U, \"Xr\": Xr, \"Ur\": Ur, \"Q\": Q, \"R\": R}}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in Function::Function for 'fc' [MXFunction] at .../casadi/core/function.cpp:235:\n.../casadi/core/function_internal.cpp:145: Error calling MXFunction::init for 'fc':\n.../casadi/core/x_function.hpp:287: For fc: Xfunction input arguments must be purely symbolic.\nArgument 0(x) is not symbolic.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000044?line=0'>1</a>\u001b[0m fc_func \u001b[39m=\u001b[39m cs\u001b[39m.\u001b[39;49mFunction(\u001b[39m'\u001b[39;49m\u001b[39mfc\u001b[39;49m\u001b[39m'\u001b[39;49m, [x_x, U], [Theta_dot_dot], [\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m], [\u001b[39m'\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py:13558\u001b[0m, in \u001b[0;36mFunction.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13465'>13466</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13466'>13467</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13467'>13468</a>\u001b[0m \n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13468'>13469</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13555'>13556</a>\u001b[0m \n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13556'>13557</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13557'>13558</a>\u001b[0m     this \u001b[39m=\u001b[39m _casadi\u001b[39m.\u001b[39;49mnew_Function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13558'>13559</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13559'>13560</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthis\u001b[39m.\u001b[39mappend(this)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in Function::Function for 'fc' [MXFunction] at .../casadi/core/function.cpp:235:\n.../casadi/core/function_internal.cpp:145: Error calling MXFunction::init for 'fc':\n.../casadi/core/x_function.hpp:287: For fc: Xfunction input arguments must be purely symbolic.\nArgument 0(x) is not symbolic."
     ]
    }
   ],
   "source": [
    "fc_func = cs.Function('fc', [x_x, U], [Theta_dot_dot], ['x', 'u'], ['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@1=vertcat(cos(theta), sin(theta)), (mac(mac((0.5*(@1-Xr)'),Q,zeros(1x2)),(@1-Xr),0)+(((0.5*(U-Ur))*R)*(U-Ur)))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in Function::Function for 'fc' [MXFunction] at .../casadi/core/function.cpp:235:\n.../casadi/core/function_internal.cpp:145: Error calling MXFunction::init for 'fc':\n.../casadi/core/x_function.hpp:287: For fc: Xfunction input arguments must be purely symbolic.\nArgument 0(x) is not symbolic.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000042?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msafe_control_gym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmath_and_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msymbolic_systems\u001b[39;00m \u001b[39mimport\u001b[39;00m SymbolicModel\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nicholasprayogo/code/rm-sc/examples/manipulator_playground.ipynb#ch0000042?line=1'>2</a>\u001b[0m symbolic \u001b[39m=\u001b[39m SymbolicModel(dynamics\u001b[39m=\u001b[39;49mdynamics, cost\u001b[39m=\u001b[39;49mcost, dt\u001b[39m=\u001b[39;49mdt)\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py:59\u001b[0m, in \u001b[0;36mSymbolicModel.__init__\u001b[0;34m(self, dynamics, cost, dt, integration_algo, funcs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=56'>57</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mUr \u001b[39m=\u001b[39m cost[\u001b[39m\"\u001b[39m\u001b[39mvars\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mUr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=57'>58</a>\u001b[0m \u001b[39m# Setup symbolic model.\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_model()\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=59'>60</a>\u001b[0m \u001b[39m# Setup Jacobian and Hessian of the dynamics and cost functions.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_linearization()\n",
      "File \u001b[0;32m~/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py:68\u001b[0m, in \u001b[0;36mSymbolicModel.setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=63'>64</a>\u001b[0m \u001b[39m\"\"\"Exposes functions to evaluate the model.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=64'>65</a>\u001b[0m \n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=65'>66</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=66'>67</a>\u001b[0m \u001b[39m# Continuous time dynamics.\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=67'>68</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_func \u001b[39m=\u001b[39m cs\u001b[39m.\u001b[39;49mFunction(\u001b[39m'\u001b[39;49m\u001b[39mfc\u001b[39;49m\u001b[39m'\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_sym, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mu_sym], [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_dot], [\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m], [\u001b[39m'\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=68'>69</a>\u001b[0m \u001b[39m# Discrete time dynamics.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=69'>70</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfd_func \u001b[39m=\u001b[39m cs\u001b[39m.\u001b[39mintegrator(\u001b[39m'\u001b[39m\u001b[39mfd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintegration_algo, {\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_sym,\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=70'>71</a>\u001b[0m                                                            \u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu_sym,\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=71'>72</a>\u001b[0m                                                            \u001b[39m'\u001b[39m\u001b[39mode\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_dot}, {\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt}\n\u001b[1;32m     <a href='file:///Users/nicholasprayogo/code/rm-sc/safe_control_gym/math_and_models/symbolic_systems.py?line=72'>73</a>\u001b[0m                             )\n",
      "File \u001b[0;32m~/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py:13558\u001b[0m, in \u001b[0;36mFunction.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13465'>13466</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13466'>13467</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13467'>13468</a>\u001b[0m \n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13468'>13469</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13555'>13556</a>\u001b[0m \n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13556'>13557</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m> <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13557'>13558</a>\u001b[0m     this \u001b[39m=\u001b[39m _casadi\u001b[39m.\u001b[39;49mnew_Function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13558'>13559</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m  <a href='file:///Users/nicholasprayogo/miniforge3/envs/sc/lib/python3.8/site-packages/casadi/casadi.py?line=13559'>13560</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthis\u001b[39m.\u001b[39mappend(this)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in Function::Function for 'fc' [MXFunction] at .../casadi/core/function.cpp:235:\n.../casadi/core/function_internal.cpp:145: Error calling MXFunction::init for 'fc':\n.../casadi/core/x_function.hpp:287: For fc: Xfunction input arguments must be purely symbolic.\nArgument 0(x) is not symbolic."
     ]
    }
   ],
   "source": [
    "from safe_control_gym.math_and_models.symbolic_systems import SymbolicModel\n",
    "symbolic = SymbolicModel(dynamics=dynamics, cost=cost, dt=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8621da41d1f75996b3114ed85820b1f2b9a1702c3804dac4519b835c2698d1e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('sc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
